<!DOCTYPE html>

<html lang="en">

<head>
  
  <title>Tags：操作系统 - Chenyu&#39;s Blog</title>
  <meta charset="UTF-8">
  <meta name="description" content="">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5">
  
  

  <link rel="shortcut icon" href="/favicon.ico" type="image/png" />
  <meta property="og:type" content="website">
<meta property="og:title" content="Chenyu&#39;s Blog">
<meta property="og:url" content="https://chenyuzhuwhiskey.github.io/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/index.html">
<meta property="og:site_name" content="Chenyu&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Chenyu Zhu">
<meta name="twitter:card" content="summary">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/combine/npm/highlight.js@9.15.8/styles/atom-one-dark.css,npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css,gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css,npm/hexo-theme-nexmoe@latest/source/lib/mdui_043tiny/css/mdui.css,npm/hexo-theme-nexmoe@latest/source/lib/iconfont/iconfont.css?v=233" crossorigin>
  <link rel="stylesheet" href="/css/style.css?v=1624344442071">
  
  <link rel="stylesheet" href="/lib/iconfont/iconfont.css?v=1624344442071">
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="mdui-drawer-body-left">
  
  <div id="nexmoe-background">
    <div class="nexmoe-bg" style="background-image: url(https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/social_app_add_code/wgwall.jpg)"></div>
    <div class="mdui-appbar mdui-shadow-0">
      <div class="mdui-toolbar">
        <a mdui-drawer="{target: '#drawer', swipe: true}" title="menu" class="mdui-btn mdui-btn-icon mdui-ripple"><i class="mdui-icon nexmoefont icon-menu"></i></a>
        <div class="mdui-toolbar-spacer"></div>
        <!--<a href="javascript:;" class="mdui-btn mdui-btn-icon"><i class="mdui-icon material-icons">search</i></a>-->
        <a href="/" title="Chenyu Zhu" class="mdui-btn mdui-btn-icon"><img src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/social_app_add_code/james-clerk-maxwell.jpg" alt="Chenyu Zhu"></a>
       </div>
    </div>
  </div>
  <div id="nexmoe-header">
      <div class="nexmoe-drawer mdui-drawer" id="drawer">
    <div class="nexmoe-avatar mdui-ripple">
        <a href="/" title="Chenyu Zhu">
            <img src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/social_app_add_code/james-clerk-maxwell.jpg" alt="Chenyu Zhu" alt="Chenyu Zhu">
        </a>
    </div>
    <div class="nexmoe-count">
        <div><span>Articles</span>12</div>
        <div><span>Tags</span>9</div>
        <div><span>Categories</span>2</div>
    </div>
    <div class="nexmoe-list mdui-list" mdui-collapse="{accordion: true}">
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/" title="Home">
            <i class="mdui-list-item-icon nexmoefont icon-home"></i>
            <div class="mdui-list-item-content">
                Home
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/archives" title="Archives">
            <i class="mdui-list-item-icon nexmoefont icon-i-catalog"></i>
            <div class="mdui-list-item-content">
                Archives
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/2020/01/01/about" title="About ME">
            <i class="mdui-list-item-icon nexmoefont icon-info-circle"></i>
            <div class="mdui-list-item-content">
                About ME
            </div>
        </a>
        
        <a class="nexmoe-list-item mdui-list-item mdui-ripple" href="/2020/01/01/sponsorship" title="Sponsorship">
            <i class="mdui-list-item-icon nexmoefont icon-coffee"></i>
            <div class="mdui-list-item-content">
                Sponsorship
            </div>
        </a>
        
    </div>
    <aside id="nexmoe-sidebar">
  
  <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-search">
        <form id="search_form" action_e="https://cn.bing.com/search?q=site:chenyuzhuwhiskey.github.io" onsubmit="return search();">
            <label><input id="search_value" name="q" type="search" placeholder="Search"></label>
        </form>
    </div>
</div>
  
  <div class="nexmoe-widget-wrap">
    <div class="nexmoe-widget nexmoe-social">
        <a class="mdui-ripple" href="http://wpa.qq.com/msgrd?v=3&uin=1241585360&site=qq&menu=yes" target="_blank" mdui-tooltip="{content: 'QQ'}" style="color: rgb(249, 174, 8);background-color: rgba(249, 174, 8, .1);">
            <i class="nexmoefont icon-QQ"></i>
        </a><a class="mdui-ripple" href="https://space.bilibili.com/12575612" target="_blank" mdui-tooltip="{content: '哔哩哔哩'}" style="color: rgb(231, 106, 141);background-color: rgba(231, 106, 141, .15);">
            <i class="nexmoefont icon-bilibili"></i>
        </a><a class="mdui-ripple" href="https://github.com/ChenyuZhuWhiskey/" target="_blank" mdui-tooltip="{content: 'GitHub'}" style="color: rgb(25, 23, 23);background-color: rgba(25, 23, 23, .15);">
            <i class="nexmoefont icon-github"></i>
        </a><a class="mdui-ripple" href="https://www.zhihu.com/people/zhu-chen-yu-9/" target="_blank" mdui-tooltip="{content: '知乎'}" style="color: rgb(12, 53, 90);background-color: rgba(30,136,229,.1);">
            <i class="nexmoefont icon-zhihu"></i>
        </a><a class="mdui-ripple" href="https://steamcommunity.com/profiles/76561198401617015/" target="_blank" mdui-tooltip="{content: 'Steam'}" style="color: rgb(5,28,63);background-color: rbga(14,71,161,.1);">
            <i class="nexmoefont icon-steam"></i>
        </a><a class="mdui-ripple" href="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/social_app_add_code/wechat.jpg" target="_blank" mdui-tooltip="{content: '微信'}" style="color: rgb(123, 179, 46);background-color: rgba(102,187,106,.1);">
            <i class="nexmoefont icon-wechat-fill"></i>
        </a><a class="mdui-ripple" href="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/social_app_add_code/wulixuetu.jpg" target="_blank" mdui-tooltip="{content: '公众号'}" style="color: ;background-color: ;">
            <i class="nexmoefont icon-wechat-fill"></i>
        </a>
    </div>
</div>
  
  
  <div class="nexmoe-widget-wrap">
    <h3 class="nexmoe-widget-title">Categories</h3>
    <div class="nexmoe-widget">

      <ul class="category-list">

        


        

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Computer-Science/">Computer Science</a>
          <span class="category-list-count">10</span>
        </li>

        

        <li class="category-list-item">
          <a class="category-list-link" href="/categories/Computer-Science/Physics/">Physics</a>
          <span class="category-list-count">1</span>
        </li>

        
      </ul>

    </div>
  </div>


  
  
  <div class="nexmoe-widget-wrap">
    <div id="randomtagcloud" class="nexmoe-widget tagcloud nexmoe-rainbow">
      <a href="/tags/Bison/" style="font-size: 10px;">Bison</a> <a href="/tags/Parser/" style="font-size: 10px;">Parser</a> <a href="/tags/Statistical-Mechanics/" style="font-size: 10px;">Statistical Mechanics</a> <a href="/tags/flex/" style="font-size: 10px;">flex</a> <a href="/tags/lexical-analysis/" style="font-size: 10px;">lexical analysis</a> <a href="/tags/translate/" style="font-size: 20px;">translate</a> <a href="/tags/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" style="font-size: 16.67px;">操作系统</a> <a href="/tags/%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/" style="font-size: 10px;">源码解析</a> <a href="/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/" style="font-size: 13.33px;">编译原理</a>
    </div>
    
  </div>

  
</aside>
    <div class="nexmoe-copyright">
        &copy; 2021 Chenyu Zhu
        Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
        & <a href="https://github.com/theme-nexmoe/hexo-theme-nexmoe" target="_blank">Nexmoe</a>
        
    </div>
</div><!-- .nexmoe-drawer -->
  </div>
  <div id="nexmoe-content">
    <div class="nexmoe-primary">
        <section class="nexmoe-posts">
    
    <div class="nexmoe-post">
        <a href="/2020/09/07/%E7%AE%80%E6%98%93%E5%86%85%E6%A0%B8%E5%AE%9E%E7%8E%B0%E7%AC%94%E8%AE%B0(%E4%BA%8C)/">
            
                <div class="nexmoe-post-cover mdui-ripple" style="padding-bottom: 62.5%;"> 
                    <img data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/social_app_add_code/wgwall.jpg" data-sizes="auto" alt="简易内核实现笔记(二)" class="lazyload">
                    <h1>简易内核实现笔记(二)</h1>
                </div>
            
        </a>

        <div class="nexmoe-post-meta nexmoe-rainbow">
            <a><i class="nexmoefont icon-calendar-fill"></i>2020年09月07日</a>
            <a><i class="nexmoefont icon-areachart"></i>4.7k 字</a>
            <a><i class="nexmoefont icon-time-circle-fill"></i>大概 19 分钟</a>
        </div>

        <article>
            
                <h1 id="简易内核实现笔记-二-——内存寻址与安全机制"><a href="#简易内核实现笔记-二-——内存寻址与安全机制" class="headerlink" title="简易内核实现笔记(二)——内存寻址与安全机制"></a>简易内核实现笔记(二)——内存寻址与安全机制</h1><p>前面叙述了x86架构的CPU在加电后是怎么一步步把内核加载到内存中去运行的，但有的东西说的比较仓促，在这里会结合硬件讲述x86架构的CPU是如何与软件结合进行内存寻址的，并且在保护模式下为内存寻址提供了哪些最基本的安全机制，有的内容会和笔记（一）重复。</p>
<h2 id="内存寻址"><a href="#内存寻址" class="headerlink" title="内存寻址"></a>内存寻址</h2><p>在x86 CPU的硬件支持下，保护模式的寻址(假如分页已经打开)是如下的过程：</p>
<pre><code>逻辑地址 --分段机制--&gt; 线性地址 --分页机制--&gt; 物理地址
</code></pre><p>可用看到就是分段与分页机制共同作用的结果。在x86架构的CPU下，分页并<strong>不是一个必要的过程</strong>，但在将控制寄存器CR0的第31位写为1开启分页模式后，分页机制就会帮助CPU实现虚拟地址空间的寻址，这样的一层抽象能够让各个进程处于好像是自己霸占了所有的内存资源一样，简化了软件对内存的访问与使用，因此linux是使用了虚拟地址空间的。但分段是x86 CPU<strong>强加的</strong>，这个下面会详细介绍。</p>
<h3 id="分段机制"><a href="#分段机制" class="headerlink" title="分段机制"></a>分段机制</h3><p>x86 CPU为段机制的实现提供了专门的寄存器：CS，DS，SS，ES，FS，GS。其中前三个看缩写也明白，分别对应了代码段(code segment)，数据段(data segment)，栈段(stack segment)。后面三个寄存器的用途x86 CPU在硬件实现上没有强加，因此功能是软件定义的，暂且不聊。我们先说前三个寄存器。</p>
<p>所谓的段(segment)，还是得从硬件的角度去理解。CPU通过总线连接了其他硬件设备，并通过总线与它们实现交互，而总线又可以分为三类：地址线，数据线，控制线。本质上说，所有的计算机信息都是二进制0和1，CPU也只认识0和1，不认识什么是代码，什么是数据，什么是地址，对于两个完全相同的01序列，CPU可用有不同的解释：</p>
<pre><code class="lang-assembly">1000100111011000 -&gt; 89DH      (数据)
1000100111011000 -&gt; mov ax,bx (代码)
</code></pre>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/02-cpu-arch.png" alt="CPU-architecture" class="lazyload"></p>
<p>CPU是通过什么机制将一些01序列视为数据，另一些01序列视为代码的呢？答案就是看它们通过什么样的线，通过数据线的就是数据，通过地址线的就是地址，通过控制线的就是指令。自然地，为了在硬件上帮助这个机制实现地更彻底一点，那就创造一种分段的机制，把内存地址空间中的01序列分成一段一段的，如果在数据段中，那这段内存中的01序列CPU就把它当成数据，处于栈段中的，CPU就把它当成栈，处于代码段中的，CPU就把它当成代码。</p>
<p>于是x86 CPU就专门设置了三个寄存器实现分段，CS存储的就是代码段的段基址，DS存储的就是数据段的段基址，SS存储的就是栈段的段基址，然后我们再给它们配套一个寄存器用来存储段的偏移地址，那么CPU只要用<code>段基址:偏移地址</code>的形式就能得到对应段的物理地址，于是就将代码，数据，栈在形式上分了开来，程序就能有条理地被执行了。</p>
<h4 id="8086分段机制"><a href="#8086分段机制" class="headerlink" title="8086分段机制"></a>8086分段机制</h4><p>由于8086在硬件架构上是有20位的地址线，也就是寻址上是1MB的内存空间，但寄存器只有16位，为了能够实现20位的寻址模式，分段机制就采用物理地址=段基址*16+偏移地址的方式来凑出20位地址。这里就可用看出分段机制不仅仅是为了将代码，数据和栈分离开，它也是8086 CPU实现20位寻址的必要机制，所以在分段上x86 家族的CPU从8086开始就深入骨髓中了。8086的运行模式在新一代的x86 CPU中也称之为实模式，所有x86家族CPU加电的瞬间都是在实模式下运行，目的就是做到向下兼容。</p>
<h4 id="保护模式分段机制"><a href="#保护模式分段机制" class="headerlink" title="保护模式分段机制"></a>保护模式分段机制</h4><p>在保护模式下，除了段寄存器之外，其余寄存器都扩展到了32位（IA32架构），那么寻址空间就从1MB变成了32位4GB，并且理论上只需要提供偏移地址的32位寄存器就可以独立完成32位的寻址，因此在保护模式下，分段机制的作用只剩下了将代码，数据和栈进行分离了。并且保护模式下硬件将与软件一起实现分段机制。</p>
<h5 id="全局描述符表"><a href="#全局描述符表" class="headerlink" title="全局描述符表"></a>全局描述符表</h5><p>抛砖引玉，我们先考虑这个问题：既然保护模式下，32位寄存器已经能在理论上脱离段寄存器独立寻址，那么段寄存器在保护模式下的意义除了向下兼容8086实模式以外还有什么？没错，答案就是为描述符表而存在！保护模式下的段寄存器存储的东西不再称为“段基址”，而是“段选择子”(selector)，而这个选择的目标就是对应的段描述符。段选择子的16位二进制结构对应的意义如下：</p>
<pre><code>|15|      ...   | 3| 2| 1| 0|
|       index      |TI| RPL |
</code></pre><p>1-0位是请求特权级，这个后面详细说。第2位是表指示符，用于指代后面的指标indexing的是GDT还是LDT，后13位就是段描述符表的index了，从长度来看总共可以索引8192个段描述符。</p>
<p>描述符表分为两种，一个是全局描述符表GDT，另外一个是局部描述符表LDT，GDT是被所有进程共享的，LDT是单个进程独有的，由于linux kernel在2.4之后并不使用LDT，这里就略过了，但它们都是一样的东西，唯一区别只是公用和私用而已。</p>
<p>所谓的全局描述符表就是一个位于内存中的描述符的数组，它的首位地址就是第一个描述符地址，一个描述符大小为8字节64位，每一位分别对应的意义如下：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/01-descriptor.png" alt="descriptor" class="lazyload"></p>
<p>可以看到这里面是有段基址的，所以保护模式的分段实际过程就是段寄存器通过存储全局描述符表基址的寄存器GDT再加上index*8寻址到对应的段描述符，然后取出对应的段基址再加上偏移地址就可以了。</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/02-seg-process.png" alt="seg-process" class="lazyload"></p>
<p>绕了大半天，其实就是为了让寻址时加上额外的一些段信息，它们意义如下，由于一些向下兼容的原因，一些东西是不连续的，但不妨碍我们理解：</p>
<ul>
<li>段界限：一个段的最大大小是20位，如果索引超过段界限CPU会触发异常。</li>
<li>G：段界限的粒度，如果G为0就代表粒度是1位，对应到段界限就是20位1MB。G为1就代表粒度为4KB，对应到段界限就是4GB，因此实际的段界限大小等=粒度大小*段界限-1</li>
<li>段基址：顾名思义，不用说了</li>
<li>D/B：一个用来兼容80286保护模式的位，表示有效地址和操作数的位数。D为0表示16位，D为1表示32位（所以对我们不用80286的就没什么用）</li>
<li>L：为1表示64位代码段，0表示32位</li>
<li>AVL：available字段，这个available是对于用户来说的，不是硬件，所以是可以随便用的</li>
<li>P：用于指示段是否存在于内存中，用到这个段时如果它不存在，就会触发CPU的异常，然后跳转到异常处理程序中把它加载到内存中。</li>
<li>DPL：Descriptor Privilege Level，表示描述符的特权级。</li>
<li>S：为1表示系统段，0表示非系统段</li>
<li>type：段的类型，这三位对于系统段和非系统段有不同的定义：</li>
</ul>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/01-descriptor-type.png" alt="descriptor-type" class="lazyload"></p>
<h4 id=""><a href="#" class="headerlink" title=" "></a> </h4><p>这些信息提供了保护模式下的安全机制，这个后面再说。BTW linux kernel认为段基址是没有意义的，因为偏移地址已经可以给出完整的线性地址，因此linux kernel的全局描述符表中的段基址位全都置为了0用以规避分段机制，因此在linux下偏移地址就等于线性地址。因此GDT对于linux存在的唯一意义就是实现内存访问的安全机制了。</p>
<h2 id="分页机制"><a href="#分页机制" class="headerlink" title="分页机制"></a>分页机制</h2><p>分页机制实际上就是将线性地址看作了虚拟地址，通过页表实现了虚拟地址到物理地址的映射，由于笔记(一)已经详细讲述，这里就直接复制粘贴了：</p>
<h3 id="虚拟地址空间"><a href="#虚拟地址空间" class="headerlink" title="虚拟地址空间"></a>虚拟地址空间</h3><p>在进入保护模式之后，我们所访问的32位地址仍然是物理地址，虚拟地址为我们提供了一层抽象，使得每个进程都可以在32位地址空间中运行，我们只需要通过页表将它们映射到物理地址即可，这样写程序就不用再自己去管地址从哪里开始了。</p>
<h4 id="页表"><a href="#页表" class="headerlink" title="页表"></a>页表</h4><p>页表是虚拟地址与物理地址的映射关系，由于将来每个操作系统下的进程，包括操作系统自己都是在32位虚拟地址空间中运行的，因此每个进程都需要有自己的页表，我们将物理地址分页，每个页占有4kB的大小，一个页表项就占32位4字节，检索4GB的虚拟内存空间总共需要1M个页表，在内存中占4MB，这个大小显然是无法接受的，因此我们再创建一个页表的页表，也就是页目录表，一个页目录项也是32位4字节，因此一个页目录项也可以索引4kB的空间，那么检索4GB的虚拟地址空间只需要4GB/4kB/4kB=1024个页目录，只需要4096个字节就可以了，这样的开销就可以接受。</p>
<p>对于1024个页目录，我们需要10位地址来进行索引，这10位地址就是虚拟地址中的高10位，我们将这10位地址<em>4就是对应页表的偏移地址，再加上页目录表的起始地址就得到了对应页表所在的物理地址，一个页表中有1024个页，因此检索它也需要10位地址，这10位地址就是虚拟地址中的中间10位，我们用这中间10位地址</em>4就得到了所在页的偏移地址，加上前面得到的页表物理地址就得到了对应页所在物理地址，这个页中存储的就是真实物理地址的偏移量，再加上最低12位虚拟地址就得到了对应的真实物理地址了。</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/02-page-process.png" alt="page-process" class="lazyload"></p>
<p>因为每个页表项都是4字节，因此它们的值里面低12位全是0，因此为了避免浪费就要往里面加一些关于页表的安全信息：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/01-page.png" alt="page" class="lazyload"></p>
<p>其中：</p>
<ul>
<li>P：该页存在于物理地址中</li>
<li>R/W：读写权限，0表示只读，1表示可读可写</li>
<li>US：普通用户/超级用户位，为1表示在普通用户级，普通用户在特权级3</li>
<li>PWT：通写位，1表示处于通写模式，表示改该页是高速缓存</li>
<li>PCD：打开使用高速缓存</li>
<li>A:访问位，如果CPU访问过该页，就会把它置为1，之后的操作系统我们会将它置为0，通过count置为1的次数就能判断它是否常常被使用，是就将这个页存入缓存中</li>
<li>D：脏页位，CPU对一个页进行写操作时，就会把这个位置为1，仅对页表项有效</li>
<li>G：global位，若为global，那么这个页表就会一直在高速缓存TLB中保存</li>
<li>AVL：软件的可用为，CPU不会管，怎么用就是软件定义的了</li>
</ul>
<h2 id="内存的安全机制"><a href="#内存的安全机制" class="headerlink" title="内存的安全机制"></a>内存的安全机制</h2><h3 id="访问特权级"><a href="#访问特权级" class="headerlink" title="访问特权级"></a>访问特权级</h3><p>除了页表项以及段描述符中的那些索引界限以及读写权限的设置以外，x86 CPU保护模式还设计了特权级来为操作系统提供安全支持。特权级从0-3一共4级，0级最高，也是操作系统内核的特权级，3级最低，是普通用户的特权级，对于linux来说，只用到了特权级0和3，因此0级特权下又称为内核态，3级特权下又称为用户态。CPU对内核态完全信任，也就是操作系统内核对硬件资源拥有完全的访问权限，低级特权无法访问被指定了高级特权能访问的硬件资源，也就是用户态的进程无法直接访问操作系统的内存空间以及代码，只能通过中断陷入内核，然后调用内核的异常处理程序来向内核请求服务，这样就保证了操作系统基本的安全。</p>
<p>那么这种机制是如何实现的呢？首先就是在段寄存器中储存的段选择子上，选择子的第1-0位上就是请求特权级，编码上的00，01，10，11就对应了0，1，2，3这四级特权级。对于栈段和数据段来说，这个特权级就代表了请求访问它们对应的段所需要的最小特权级，而对于代码段来说，这个特权级就代表了这段代码执行的特权级，因此代码段的RPL叫CPL(current privilege level)，也就是当前指令的特权级。前面说描述符的时候有提到，描述符里也有它自己的特权级DPL，因此DPL也在安全特权检查之列。</p>
<p>在CS:EIP指向了内存中的一个指令地址的时候，如果不考虑特权级转换，CPU会做的完整步骤如下：</p>
<ul>
<li>首先根据CS的index检索到对应代码段的段描述符，得到描述符的DPL，然后用CS的CPL比较，如果CPL&gt;DPL，则报保护错(数字越小特权越高)</li>
<li>CPL大于等于代码段描述符DPL，则通过描述符提供的段基址+EIP的偏移地址得到指令的线性虚拟地址，然后通过页表缓存或页表查询到物理地址，取指令执行</li>
<li>指令执行时会如果访问到相应的数据段或者栈段，则对应段选择则先indexing到对应的段描述符，然后检查保证CPL或者访问段选择子的RPL有一个小于等于该段描述符的DPL，如果max{CPL,RPL}&gt;DPL，则报保护错</li>
<li>检查通过，然后访问相应资源，指令执行完毕后加载下一条指令跳回第一步</li>
</ul>
<h3 id="特权级的提升与降低"><a href="#特权级的提升与降低" class="headerlink" title="特权级的提升与降低"></a>特权级的提升与降低</h3><p>CPU还要考虑陷入内核态后上下文的保存问题，进程触发异常后会陷入内核态，然后内核调用相应的异常处理程序，此时特权级就从3提升到0，在执行完内核代码之后（如果不是终止异常）又返回用户态。那么一个进程从3跳到0的过程要有4组栈寄存器来对应每个特权级的栈段和栈底。32位机器下4GB的寻址空间中最高位的1GB是内核才能访问的，这里面就有内核使用的栈段，肯定要和用户用的低3GB地址下的栈段区分，并且在进程陷入内核态之时，用户态的上下文信息肯定要保存下来，等待内核代码做完事情以后恢复现场。实现的方法就是一个叫TSS(task state segment)的数据结构：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/02-tss.png" alt="02-TSS" class="lazyload"></p>
<p>可以看到TSS只能保留3组栈，因为用户态本来就是最低权限，已经降不下去了，，而且汇编指令的<code>int,call</code>会将用户态的栈保存，TSS就只用记录0，1，2这三个特权级的栈寄存器就OK。每个进程都有自己的TSS，并且x86 CPU会有专门的寄存器TR(task register)来保存它的地址，当用户态的进程陷入内核态时，除了SS，ESP以外的上下文信息就会被保存，然后使用0级特权栈配合CRL为0的内核代码完成相应异常处理程序，最后再恢复现场，把特权级降回用户态就完事了。</p>
<p>降低特权级可以通过恢复进程上下文实现，但还得考虑怎么提升特权级的问题。CPU又提供了一组和硬件支持的数据结构来实现，这种数据结构称为‘门’。一共四种，任务门，中断门，陷阱门和调用门：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/02-task-gate.png" alt="task-gate" class="lazyload"></p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/02-interrupt-gate.png" alt="intr-gate" class="lazyload"></p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/02-trap-gate.png" alt="trap-gate" class="lazyload"></p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/02-call-gate.png" alt="call-gate" class="lazyload"></p>
<p>门也是一种描述符，只不过和全局描述符表中的描述符不一样的是，全局描述符表是记录数据的描述符，而门中除了任务门以外记录的是一段历程地址的描述符，用于支持内核的系统调用：</p>
<ul>
<li>call和jmp指令的选择子会成为调用门的参数，指令CPL通过调用门的DPL特权检查后call会以调用高CPL函数例程形式实现特权级提升，jmp只能转移到CPL平行的代码上。</li>
<li>int指令会触发中断，指令CPL通过中断门的DPL特权检查后，linux并根据中断类型调用相应的异常处理程序，然后以中断形式进入内核态实现特权提升。</li>
<li>int3指令通过触发中断形式在陷阱门中实现特权提升，一般是编译器调试用，不用管</li>
<li>任务(进程)在中断发生后如果中断向量号是任务门，则通过任务门以TSS为单位实现任务切换，不过linux并没有使用这样的硬件机制，所以不用管</li>
</ul>
<p>综上，一个指令在执行的时候，它的CPL必须满足以下条件：</p>
<ul>
<li>访问门（向内核请求服务）：CPL≤DPL(gate) and CPL≥DPL(seg)</li>
<li>访问段：max{CPL,RPL}≤DPL</li>
</ul>
<p>这就是特权在保护模式下提供的安全机制，可见这些安全机制一部分是由硬件实现，一部分是由操作系统内核实现的。</p>

            
        </article>
    </div>
    
    <div class="nexmoe-post">
        <a href="/2020/09/06/%E7%AE%80%E6%98%93%E5%86%85%E6%A0%B8%E5%AE%9E%E7%8E%B0%E7%AC%94%E8%AE%B0(%E4%B8%80)/">
            
                <div class="nexmoe-post-cover mdui-ripple" style="padding-bottom: 62.5%;"> 
                    <img data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/social_app_add_code/wgwall.jpg" data-sizes="auto" alt="简易内核实现笔记(一)" class="lazyload">
                    <h1>简易内核实现笔记(一)</h1>
                </div>
            
        </a>

        <div class="nexmoe-post-meta nexmoe-rainbow">
            <a><i class="nexmoefont icon-calendar-fill"></i>2020年09月06日</a>
            <a><i class="nexmoefont icon-areachart"></i>7.5k 字</a>
            <a><i class="nexmoefont icon-time-circle-fill"></i>大概 34 分钟</a>
        </div>

        <article>
            
                <h1 id="简易内核实现笔记-一-——开启操作系统前的准备"><a href="#简易内核实现笔记-一-——开启操作系统前的准备" class="headerlink" title="简易内核实现笔记(一) ——开启操作系统前的准备"></a>简易内核实现笔记(一) ——开启操作系统前的准备</h1><h2 id="BIOS"><a href="#BIOS" class="headerlink" title="BIOS"></a>BIOS</h2><p>在计算机电源打开的一瞬间，x86架构的CPU处于实模式下，所谓的实模式就是8086CPU运行的模式，x86家族的CPU为了做到向下兼容，全部默认开机时运行在8086的模式下，在实模式中，所有的地址都是物理地址，寄存器大小都是16位，寻址采用20位地址线，由段地址左移4位+偏移地址实现。</p>
<p>在实模式背景下，第一行代码的位置是0xf000：0xfff0，也就是0xffff0，这一行代码的指令是<code>jmp f000:e05b</code>,这个跳转的地址就是BIOS的第一行代码地址，随后BIOS就会进行硬件自检，在没有问题后就会执行最后一行代码<code>jmp 0x7c00</code>跳转到主引导程序MBR处。</p>
<h2 id="MBR"><a href="#MBR" class="headerlink" title="MBR"></a>MBR</h2><p>MBR占512字节，正好是一个硬盘扇区的大小，在这512字节的程序中，MBR的任务就是把加载器载入内存中执行：</p>
<pre><code class="lang-assembly">;主引导程序 
;------------------------------------------------------------
%include &quot;boot.inc&quot;
SECTION MBR vstart=0x7c00         
   mov ax,cs      
   mov ds,ax
   mov es,ax
   mov ss,ax
   mov fs,ax
   mov sp,0x7c00
   mov ax,0xb800
   mov gs,ax

; 清屏
;利用0x06号功能，上卷全部行，则可清屏。
; -----------------------------------------------------------
;INT 0x10   功能号:0x06       功能描述:上卷窗口
;------------------------------------------------------
;输入：
;AH 功能号= 0x06
;AL = 上卷的行数(如果为0,表示全部)
;BH = 上卷行属性
;(CL,CH) = 窗口左上角的(X,Y)位置
;(DL,DH) = 窗口右下角的(X,Y)位置
;无返回值：
   mov     ax, 0600h
   mov     bx, 0700h
   mov     cx, 0                   ; 左上角: (0, 0)
   mov     dx, 184fh           ; 右下角: (80,25),
                   ; 因为VGA文本模式中，一行只能容纳80个字符,共25行。
                   ; 下标从0开始，所以0x18=24,0x4f=79
   int     10h                     ; int 10h

   ; 输出字符串:MBR
   mov byte [gs:0x00],&#39;1&#39;
   mov byte [gs:0x01],0xA4

   mov byte [gs:0x02],&#39; &#39;
   mov byte [gs:0x03],0xA4

   mov byte [gs:0x04],&#39;M&#39;
   mov byte [gs:0x05],0xA4       ;A表示绿色背景闪烁，4表示前景色为红色

   mov byte [gs:0x06],&#39;B&#39;
   mov byte [gs:0x07],0xA4

   mov byte [gs:0x08],&#39;R&#39;
   mov byte [gs:0x09],0xA4

   mov eax,LOADER_START_SECTOR     ; 起始扇区lba地址
   mov bx,LOADER_BASE_ADDR       ; 写入的地址
   mov cx,4             ; 待读入的扇区数
   call rd_disk_m_16         ; 以下读取程序的起始部分（一个扇区）

   jmp LOADER_BASE_ADDR + 0x300

;-------------------------------------------------------------------------------
;功能:读取硬盘n个扇区
rd_disk_m_16:       
;-------------------------------------------------------------------------------
                       ; eax=LBA扇区号
                       ; ebx=将数据写入的内存地址
                       ; ecx=读入的扇区数
      mov esi,eax      ;备份eax
      mov di,cx          ;备份cx
;读写硬盘:
;第1步：设置要读取的扇区数
      mov dx,0x1f2
      mov al,cl
      out dx,al            ;读取的扇区数

      mov eax,esi       ;恢复ax

;第2步：将LBA地址存入0x1f3 ~ 0x1f6

      ;LBA地址7~0位写入端口0x1f3
      mov dx,0x1f3                       
      out dx,al                          

      ;LBA地址15~8位写入端口0x1f4
      mov cl,8
      shr eax,cl
      mov dx,0x1f4
      out dx,al

      ;LBA地址23~16位写入端口0x1f5
      shr eax,cl
      mov dx,0x1f5
      out dx,al

      shr eax,cl
      and al,0x0f       ;lba第24~27位
      or al,0xe0       ; 设置7～4位为1110,表示lba模式
      mov dx,0x1f6
      out dx,al

;第3步：向0x1f7端口写入读命令，0x20 
      mov dx,0x1f7
      mov al,0x20                        
      out dx,al

;第4步：检测硬盘状态
  .not_ready:
      ;同一端口，写时表示写入命令字，读时表示读入硬盘状态
      nop
      in al,dx
      and al,0x88       ;第4位为1表示硬盘控制器已准备好数据传输，第7位为1表示硬盘忙
      cmp al,0x08
      jnz .not_ready       ;若未准备好，继续等。

;第5步：从0x1f0端口读数据
      mov ax, di
      mov dx, 256
      mul dx
      mov cx, ax       ; di为要读取的扇区数，一个扇区有512字节，每次读入一个字，
               ; 共需di*512/2次，所以di*256
      mov dx, 0x1f0
  .go_on_read:
      in ax,dx
      mov [bx],ax
      add bx,2          
      loop .go_on_read
      ret

   times 510-($-$$) db 0
   db 0x55,0xaa
</code></pre>
<p>可以看到MBR的代码分为两部分，第一个部分就是在窗口打印”1 MBR”这几个字符，这是通过向段起始0xb800处的内存写入字符实现的。在实模式下，这个地址就是显存的位置。第二部分就是写入loader，也就是函数<code>rd_disk_m_16</code>，在这个函数里，<code>cx</code>寄存器储存的是要读的磁盘扇区个数。相关的宏定义如下：</p>
<p>宏<code>LOADER_START_SECTOR</code>就是0x2，表示我们要向磁盘第三个扇区（第一个是0x0）读loader，<code>LOADER_BASE_ADDR</code>就是loader被写入的地址0x900。</p>
<p>在加载完loader之后，MBR的使命就结束了，最后一条命令<code>jmpLOADER_BASE_ADDR+0x300</code>就是跳转到loader的第一条命令去执行loader。</p>
<h2 id="Loader"><a href="#Loader" class="headerlink" title="Loader"></a>Loader</h2><p>我们的loader就负责做四个事情：</p>
<ul>
<li>加载全局描述符表</li>
<li>进入保护模式</li>
<li>创建页表，展开虚拟地址空间</li>
<li>加载操作系统内核</li>
</ul>
<h3 id="保护模式"><a href="#保护模式" class="headerlink" title="保护模式"></a>保护模式</h3><p>所谓的保护模式就是可以寻址32位（4GB）的模式，而’保护’二字指的就是在这个模式下CPU为程序执行提供了一些内存的保护措施，这个措施就是通过全局描述符表来实现的。为了开启保护模式，我们要做3件事：</p>
<ul>
<li>加载全局描述符表</li>
<li>打开A20 Gate</li>
<li>修改控制寄存器CR0第一位为1</li>
</ul>
<h4 id="全局描述符表"><a href="#全局描述符表" class="headerlink" title="全局描述符表"></a>全局描述符表</h4><p>全局描述符表就是一个表，存储着段描述符，所谓的描述符就是关于内存段的一些信息，CPU会根据这些信息做出相应的措施，所谓的全局就是指这个表不是局部的。一个描述符占了64位8字节，每位的意义如下：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/01-descriptor.png" alt="descriptor" class="lazyload"></p>
<p>由于一些向下兼容的原因，一些东西是不连续的，但不妨碍我们理解：</p>
<ul>
<li>段界限：一个段的最大大小是20位，如果索引超过段界限CPU会触发异常。</li>
<li>G：段界限的粒度，如果G为0就代表粒度是1位，对应到段界限就是20位1MB。G为1就代表粒度为4KB，对应到段界限就是4GB，因此实际的段界限大小等=粒度大小*段界限-1</li>
<li>段基址：顾名思义，不用说了</li>
<li>D/B：一个用来兼容80286保护模式的位，表示有效地址和操作数的位数。D为0表示16位，D为1表示32位（所以对我们不用80286的就没什么用）</li>
<li>L：为1表示64位代码段，0表示32位</li>
<li>AVL：available字段，这个available是对于用户来说的，不是硬件，所以是可以随便用的</li>
<li>P：用于指示段是否存在于内存中，用到这个段时如果它不存在，就会触发CPU的异常，然后跳转到异常处理程序中把它加载到内存中。</li>
<li>DPL：表示特权级，特权级一共4级，从高到低为0，1，2，3。</li>
<li>S：为1表示系统段，0表示非系统段</li>
<li>type：段的类型，这三位对于系统段和非系统段有不同的定义：</li>
</ul>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/01-descriptor-type.png" alt="descriptor-type" class="lazyload"></p>
<h4 id="A20-Gate"><a href="#A20-Gate" class="headerlink" title="A20 Gate"></a>A20 Gate</h4><p>实模式能够寻址的空间是1MB 20位，要进入保护模式的32位寻址，就要去除20位寻址的限制，这个限制被称为A20 Gate，打开A20 Gate的方法就是将端口0x92的第一个位置写为1：</p>
<pre><code class="lang-assembly">in al,0x92
or al,0000_0010B
out 0x92,al
</code></pre>
<p>而进入保护模式的方法就是将控制寄存器CR0的第0位写为1：</p>
<pre><code class="lang-assembly">mov eax, cr0
or eax, 0x00000001
mov cr0, eax
</code></pre>
<p>因此，进入保护模式的代码如下：</p>
<pre><code class="lang-assembly">;-----------------   准备进入保护模式   -------------------
;1 打开A20
;2 加载gdt
;3 将cr0的pe位置1

   ;-----------------  打开A20  ----------------
   in al,0x92
   or al,0000_0010B
   out 0x92,al

   ;-----------------  加载GDT  ----------------
   lgdt [gdt_ptr]

   ;-----------------  cr0第0位置1  ----------------
   mov eax, cr0
   or eax, 0x00000001
   mov cr0, eax

   jmp dword SELECTOR_CODE:p_mode_start         ; 刷新流水线，避免分支预测的影响,这种cpu优化策略，最怕jmp跳转，
                         ; 这将导致之前做的预测失效，从而起到了刷新的作用。
.error_hlt:              ;出错则挂起
   hlt
</code></pre>
<h3 id="虚拟地址空间"><a href="#虚拟地址空间" class="headerlink" title="虚拟地址空间"></a>虚拟地址空间</h3><p>在进入保护模式之后，我们所访问的32位地址仍然是物理地址，虚拟地址为我们提供了一层抽象，使得每个进程都可以在32位地址空间中运行，我们只需要通过页表将它们映射到物理地址即可，这样写程序就不用再自己去管地址从哪里开始了。</p>
<h4 id="页表"><a href="#页表" class="headerlink" title="页表"></a>页表</h4><p>页表是虚拟地址与物理地址的映射关系，由于将来每个操作系统下的进程，包括操作系统自己都是在32位虚拟地址空间中运行的，因此每个进程都需要有自己的页表，我们将物理地址分页，每个页占有4kB的大小，一个页表项就占32位4字节，检索4GB的虚拟内存空间总共需要1M个页表，在内存中占4MB，这个大小显然是无法接受的，因此我们再创建一个页表的页表，也就是页目录表，一个页目录项也是32位4字节，因此一个页目录项也可以索引4kB的空间，那么检索4GB的虚拟地址空间只需要4GB/4kB/4kB=1024个页目录，只需要4096个字节就可以了，这样的开销就可以接受。</p>
<p>对于1024个页目录，我们需要10位地址来进行索引，这10位地址就是虚拟地址中的高10位，我们将这10位地址<em>4就是对应页表的偏移地址，再加上页目录表的起始地址就得到了对应页表所在的物理地址，一个页表中有1024个页，因此检索它也需要10位地址，这10位地址就是虚拟地址中的中间10位，我们用这中间10位地址</em>4就得到了所在页的偏移地址，加上前面得到的页表物理地址就得到了对应页所在物理地址，这个页中存储的就是真实物理地址的偏移量，再加上最低12位虚拟地址就得到了对应的真实物理地址了。</p>
<p>因为每个页表项都是4字节，因此它们的值里面低12位全是0，因此为了避免浪费就要往里面加一些关于页表的安全信息：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/01-page.png" alt="page" class="lazyload"></p>
<p>其中：</p>
<ul>
<li>P：该页存在于物理地址中</li>
<li>R/W：读写权限，0表示只读，1表示可读可写</li>
<li>US：普通用户/超级用户位，为1表示在普通用户级，普通用户在特权级3</li>
<li>PWT：通写位，1表示处于通写模式，表示改该页是高速缓存</li>
<li>PCD：打开使用高速缓存</li>
<li>A:访问位，如果CPU访问过该页，就会把它置为1，之后的操作系统我们会将它置为0，通过count置为1的次数就能判断它是否常常被使用，是就将这个页存入缓存中</li>
<li>D：脏页位，CPU对一个页进行写操作时，就会把这个位置为1，仅对页表项有效</li>
<li>G：global位，若为global，那么这个页表就会一直在高速缓存TLB中保存</li>
<li>AVL：软件的可用为，CPU不会管，怎么用就是软件定义的了</li>
</ul>
<p>对页表的初始化我们要有一个约定，也就是4GB的虚拟地址空间中，高1GB是只有操作系统内核才能访问的区域，因此在初始化页表时我们要将内核区的页表和普通页表分开，并且为了减小开销在未来将所有进程的内核页表通用，所以完整的loader代码如下：</p>
<pre><code class="lang-assembly">   %include &quot;boot.inc&quot;
   section loader vstart=LOADER_BASE_ADDR
;构建gdt及其内部的描述符
   GDT_BASE:   dd    0x00000000 
           dd    0x00000000

   CODE_DESC:  dd    0x0000FFFF 
           dd    DESC_CODE_HIGH4

   DATA_STACK_DESC:  dd    0x0000FFFF
             dd    DESC_DATA_HIGH4

   VIDEO_DESC: dd    0x80000007           ; limit=(0xbffff-0xb8000)/4k=0x7
           dd    DESC_VIDEO_HIGH4  ; 此时dpl为0

   GDT_SIZE   equ   $ - GDT_BASE
   GDT_LIMIT   equ   GDT_SIZE -    1 
   times 60 dq 0                     ; 此处预留60个描述符的空位(slot)
   SELECTOR_CODE equ (0x0001&lt;&lt;3) + TI_GDT + RPL0         ; 相当于(CODE_DESC - GDT_BASE)/8 + TI_GDT + RPL0
   SELECTOR_DATA equ (0x0002&lt;&lt;3) + TI_GDT + RPL0     ; 同上
   SELECTOR_VIDEO equ (0x0003&lt;&lt;3) + TI_GDT + RPL0     ; 同上 

   ; total_mem_bytes用于保存内存容量,以字节为单位,此位置比较好记。
   ; 当前偏移loader.bin文件头0x200字节,loader.bin的加载地址是0x900,
   ; 故total_mem_bytes内存中的地址是0xb00.将来在内核中咱们会引用此地址
   total_mem_bytes dd 0                     
   ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

   ;以下是定义gdt的指针，前2字节是gdt界限，后4字节是gdt起始地址
   gdt_ptr  dw  GDT_LIMIT 
        dd  GDT_BASE

   ;人工对齐:total_mem_bytes4字节+gdt_ptr6字节+ards_buf244字节+ards_nr2,共256字节
   ards_buf times 244 db 0
   ards_nr dw 0              ;用于记录ards结构体数量

   loader_start:

;-------  int 15h eax = 0000E820h ,edx = 534D4150h (&#39;SMAP&#39;) 获取内存布局  -------

   xor ebx, ebx              ;第一次调用时，ebx值要为0
   mov edx, 0x534d4150          ;edx只赋值一次，循环体中不会改变
   mov di, ards_buf          ;ards结构缓冲区
.e820_mem_get_loop:          ;循环获取每个ARDS内存范围描述结构
   mov eax, 0x0000e820          ;执行int 0x15后,eax值变为0x534d4150,所以每次执行int前都要更新为子功能号。
   mov ecx, 20              ;ARDS地址范围描述符结构大小是20字节
   int 0x15
   jc .e820_failed_so_try_e801   ;若cf位为1则有错误发生，尝试0xe801子功能
   add di, cx              ;使di增加20字节指向缓冲区中新的ARDS结构位置
   inc word [ards_nr]          ;记录ARDS数量
   cmp ebx, 0              ;若ebx为0且cf不为1,这说明ards全部返回，当前已是最后一个
   jnz .e820_mem_get_loop

;在所有ards结构中，找出(base_add_low + length_low)的最大值，即内存的容量。
   mov cx, [ards_nr]          ;遍历每一个ARDS结构体,循环次数是ARDS的数量
   mov ebx, ards_buf 
   xor edx, edx              ;edx为最大的内存容量,在此先清0
.find_max_mem_area:          ;无须判断type是否为1,最大的内存块一定是可被使用
   mov eax, [ebx]          ;base_add_low
   add eax, [ebx+8]          ;length_low
   add ebx, 20              ;指向缓冲区中下一个ARDS结构
   cmp edx, eax              ;冒泡排序，找出最大,edx寄存器始终是最大的内存容量
   jge .next_ards
   mov edx, eax              ;edx为总内存大小
.next_ards:
   loop .find_max_mem_area
   jmp .mem_get_ok

;------  int 15h ax = E801h 获取内存大小,最大支持4G  ------
; 返回后, ax cx 值一样,以KB为单位,bx dx值一样,以64KB为单位
; 在ax和cx寄存器中为低16M,在bx和dx寄存器中为16MB到4G。
.e820_failed_so_try_e801:
   mov ax,0xe801
   int 0x15
   jc .e801_failed_so_try88   ;若当前e801方法失败,就尝试0x88方法

;1 先算出低15M的内存,ax和cx中是以KB为单位的内存数量,将其转换为以byte为单位
   mov cx,0x400         ;cx和ax值一样,cx用做乘数
   mul cx 
   shl edx,16
   and eax,0x0000FFFF
   or edx,eax
   add edx, 0x100000 ;ax只是15MB,故要加1MB
   mov esi,edx         ;先把低15MB的内存容量存入esi寄存器备份

;2 再将16MB以上的内存转换为byte为单位,寄存器bx和dx中是以64KB为单位的内存数量
   xor eax,eax
   mov ax,bx        
   mov ecx, 0x10000    ;0x10000十进制为64KB
   mul ecx        ;32位乘法,默认的被乘数是eax,积为64位,高32位存入edx,低32位存入eax.
   add esi,eax        ;由于此方法只能测出4G以内的内存,故32位eax足够了,edx肯定为0,只加eax便可
   mov edx,esi        ;edx为总内存大小
   jmp .mem_get_ok

;-----------------  int 15h ah = 0x88 获取内存大小,只能获取64M之内  ----------
.e801_failed_so_try88: 
   ;int 15后，ax存入的是以kb为单位的内存容量
   mov  ah, 0x88
   int  0x15
   jc .error_hlt
   and eax,0x0000FFFF

   ;16位乘法，被乘数是ax,积为32位.积的高16位在dx中，积的低16位在ax中
   mov cx, 0x400     ;0x400等于1024,将ax中的内存容量换为以byte为单位
   mul cx
   shl edx, 16         ;把dx移到高16位
   or edx, eax         ;把积的低16位组合到edx,为32位的积
   add edx,0x100000  ;0x88子功能只会返回1MB以上的内存,故实际内存大小要加上1MB

.mem_get_ok:
   mov [total_mem_bytes], edx     ;将内存换为byte单位后存入total_mem_bytes处。


;-----------------   准备进入保护模式   -------------------
;1 打开A20
;2 加载gdt
;3 将cr0的pe位置1

   ;-----------------  打开A20  ----------------
   in al,0x92
   or al,0000_0010B
   out 0x92,al

   ;-----------------  加载GDT  ----------------
   lgdt [gdt_ptr]

   ;-----------------  cr0第0位置1  ----------------
   mov eax, cr0
   or eax, 0x00000001
   mov cr0, eax

   jmp dword SELECTOR_CODE:p_mode_start         ; 刷新流水线，避免分支预测的影响,这种cpu优化策略，最怕jmp跳转，
                         ; 这将导致之前做的预测失效，从而起到了刷新的作用。
.error_hlt:              ;出错则挂起
   hlt

[bits 32]
p_mode_start:
   mov ax, SELECTOR_DATA
   mov ds, ax
   mov es, ax
   mov ss, ax
   mov esp,LOADER_STACK_TOP
   mov ax, SELECTOR_VIDEO
   mov gs, ax

; -------------------------   加载kernel  ----------------------
   mov eax, KERNEL_START_SECTOR        ; kernel.bin所在的扇区号
   mov ebx, KERNEL_BIN_BASE_ADDR       ; 从磁盘读出后，写入到ebx指定的地址
   mov ecx, 200                   ; 读入的扇区数

   call rd_disk_m_32

   ; 创建页目录及页表并初始化页内存位图
   call setup_page

   ;要将描述符表地址及偏移量写入内存gdt_ptr,一会用新地址重新加载
   sgdt [gdt_ptr]          ; 存储到原来gdt所有的位置

   ;将gdt描述符中视频段描述符中的段基址+0xc0000000
   mov ebx, [gdt_ptr + 2]  
   or dword [ebx + 0x18 + 4], 0xc0000000      ;视频段是第3个段描述符,每个描述符是8字节,故0x18。
                          ;段描述符的高4字节的最高位是段基址的31~24位

   ;将gdt的基址加上0xc0000000使其成为内核所在的高地址
   add dword [gdt_ptr + 2], 0xc0000000

   add esp, 0xc0000000        ; 将栈指针同样映射到内核地址

   ; 把页目录地址赋给cr3
   mov eax, PAGE_DIR_TABLE_POS
   mov cr3, eax

   ; 打开cr0的pg位(第31位)
   mov eax, cr0
   or eax, 0x80000000
   mov cr0, eax

   ;在开启分页后,用gdt新的地址重新加载
   lgdt [gdt_ptr]             ; 重新加载

;;;;;;;;;;;;;;;;;;;;;;;;;;;;  此时不刷新流水线也没问题  ;;;;;;;;;;;;;;;;;;;;;;;;
;由于一直处在32位下,原则上不需要强制刷新,经过实际测试没有以下这两句也没问题.
;但以防万一，还是加上啦，免得将来出来莫句奇妙的问题.
   jmp SELECTOR_CODE:enter_kernel      ;强制刷新流水线,更新gdt
enter_kernel:    
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
   call kernel_init
   mov esp, 0xc009f000
   jmp KERNEL_ENTRY_POINT                 ; 用地址0x1500访问测试，结果ok


;-----------------   将kernel.bin中的segment拷贝到编译的地址   -----------
kernel_init:
   xor eax, eax
   xor ebx, ebx        ;ebx记录程序头表地址
   xor ecx, ecx        ;cx记录程序头表中的program header数量
   xor edx, edx        ;dx 记录program header尺寸,即e_phentsize

   mov dx, [KERNEL_BIN_BASE_ADDR + 42]      ; 偏移文件42字节处的属性是e_phentsize,表示program header大小
   mov ebx, [KERNEL_BIN_BASE_ADDR + 28]   ; 偏移文件开始部分28字节的地方是e_phoff,表示第1 个program header在文件中的偏移量
                      ; 其实该值是0x34,不过还是谨慎一点，这里来读取实际值
   add ebx, KERNEL_BIN_BASE_ADDR
   mov cx, [KERNEL_BIN_BASE_ADDR + 44]    ; 偏移文件开始部分44字节的地方是e_phnum,表示有几个program header
.each_segment:
   cmp byte [ebx + 0], PT_NULL          ; 若p_type等于 PT_NULL,说明此program header未使用。
   je .PTNULL

   ;为函数memcpy压入参数,参数是从右往左依然压入.函数原型类似于 memcpy(dst,src,size)
   push dword [ebx + 16]          ; program header中偏移16字节的地方是p_filesz,压入函数memcpy的第三个参数:size
   mov eax, [ebx + 4]              ; 距程序头偏移量为4字节的位置是p_offset
   add eax, KERNEL_BIN_BASE_ADDR      ; 加上kernel.bin被加载到的物理地址,eax为该段的物理地址
   push eax                  ; 压入函数memcpy的第二个参数:源地址
   push dword [ebx + 8]              ; 压入函数memcpy的第一个参数:目的地址,偏移程序头8字节的位置是p_vaddr，这就是目的地址
   call mem_cpy                  ; 调用mem_cpy完成段复制
   add esp,12                  ; 清理栈中压入的三个参数
.PTNULL:
   add ebx, edx                  ; edx为program header大小,即e_phentsize,在此ebx指向下一个program header 
   loop .each_segment
   ret

;----------  逐字节拷贝 mem_cpy(dst,src,size) ------------
;输入:栈中三个参数(dst,src,size)
;输出:无
;---------------------------------------------------------
mem_cpy:              
   cld
   push ebp
   mov ebp, esp
   push ecx           ; rep指令用到了ecx，但ecx对于外层段的循环还有用，故先入栈备份
   mov edi, [ebp + 8]       ; dst
   mov esi, [ebp + 12]       ; src
   mov ecx, [ebp + 16]       ; size
   rep movsb           ; 逐字节拷贝

   ;恢复环境
   pop ecx        
   pop ebp
   ret


;-------------   创建页目录及页表   ---------------
setup_page:
;先把页目录占用的空间逐字节清0
   mov ecx, 4096
   mov esi, 0
.clear_page_dir:
   mov byte [PAGE_DIR_TABLE_POS + esi], 0
   inc esi
   loop .clear_page_dir

;开始创建页目录项(PDE)
.create_pde:                     ; 创建Page Directory Entry
   mov eax, PAGE_DIR_TABLE_POS
   add eax, 0x1000                  ; 此时eax为第一个页表的位置及属性
   mov ebx, eax                     ; 此处为ebx赋值，是为.create_pte做准备，ebx为基址。

;   下面将页目录项0和0xc00都存为第一个页表的地址，
;   一个页表可表示4MB内存,这样0xc03fffff以下的地址和0x003fffff以下的地址都指向相同的页表，
;   这是为将地址映射为内核地址做准备
   or eax, PG_US_U | PG_RW_W | PG_P         ; 页目录项的属性RW和P位为1,US为1,表示用户属性,所有特权级别都可以访问.
   mov [PAGE_DIR_TABLE_POS + 0x0], eax       ; 第1个目录项,在页目录表中的第1个目录项写入第一个页表的位置(0x101000)及属性(3)
   mov [PAGE_DIR_TABLE_POS + 0xc00], eax     ; 一个页表项占用4字节,0xc00表示第768个页表占用的目录项,0xc00以上的目录项用于内核空间,
                         ; 也就是页表的0xc0000000~0xffffffff共计1G属于内核,0x0~0xbfffffff共计3G属于用户进程.
   sub eax, 0x1000
   mov [PAGE_DIR_TABLE_POS + 4092], eax         ; 使最后一个目录项指向页目录表自己的地址

;下面创建页表项(PTE)
   mov ecx, 256                     ; 1M低端内存 / 每页大小4k = 256
   mov esi, 0
   mov edx, PG_US_U | PG_RW_W | PG_P         ; 属性为7,US=1,RW=1,P=1
.create_pte:                     ; 创建Page Table Entry
   mov [ebx+esi*4],edx                 ; 此时的ebx已经在上面通过eax赋值为0x101000,也就是第一个页表的地址 
   add edx,4096
   inc esi
   loop .create_pte

;创建内核其它页表的PDE
   mov eax, PAGE_DIR_TABLE_POS
   add eax, 0x2000              ; 此时eax为第二个页表的位置
   or eax, PG_US_U | PG_RW_W | PG_P  ; 页目录项的属性RW和P位为1,US为0
   mov ebx, PAGE_DIR_TABLE_POS
   mov ecx, 254                 ; 范围为第769~1022的所有目录项数量
   mov esi, 769
.create_kernel_pde:
   mov [ebx+esi*4], eax
   inc esi
   add eax, 0x1000
   loop .create_kernel_pde
   ret


;-------------------------------------------------------------------------------
               ;功能:读取硬盘n个扇区
rd_disk_m_32:       
;-------------------------------------------------------------------------------
                             ; eax=LBA扇区号
                             ; ebx=将数据写入的内存地址
                             ; ecx=读入的扇区数
      mov esi,eax       ; 备份eax
      mov di,cx           ; 备份扇区数到di
;读写硬盘:
;第1步：设置要读取的扇区数
      mov dx,0x1f2
      mov al,cl
      out dx,al            ;读取的扇区数

      mov eax,esi       ;恢复ax

;第2步：将LBA地址存入0x1f3 ~ 0x1f6

      ;LBA地址7~0位写入端口0x1f3
      mov dx,0x1f3                       
      out dx,al                          

      ;LBA地址15~8位写入端口0x1f4
      mov cl,8
      shr eax,cl
      mov dx,0x1f4
      out dx,al

      ;LBA地址23~16位写入端口0x1f5
      shr eax,cl
      mov dx,0x1f5
      out dx,al

      shr eax,cl
      and al,0x0f       ;lba第24~27位
      or al,0xe0       ; 设置7～4位为1110,表示lba模式
      mov dx,0x1f6
      out dx,al

;第3步：向0x1f7端口写入读命令，0x20 
      mov dx,0x1f7
      mov al,0x20                        
      out dx,al

;;;;;;; 至此,硬盘控制器便从指定的lba地址(eax)处,读出连续的cx个扇区,下面检查硬盘状态,不忙就能把这cx个扇区的数据读出来

;第4步：检测硬盘状态
  .not_ready:           ;测试0x1f7端口(status寄存器)的的BSY位
      ;同一端口,写时表示写入命令字,读时表示读入硬盘状态
      nop
      in al,dx
      and al,0x88       ;第4位为1表示硬盘控制器已准备好数据传输,第7位为1表示硬盘忙
      cmp al,0x08
      jnz .not_ready       ;若未准备好,继续等。

;第5步：从0x1f0端口读数据
      mov ax, di       ;以下从硬盘端口读数据用insw指令更快捷,不过尽可能多的演示命令使用,
               ;在此先用这种方法,在后面内容会用到insw和outsw等

      mov dx, 256       ;di为要读取的扇区数,一个扇区有512字节,每次读入一个字,共需di*512/2次,所以di*256
      mul dx
      mov cx, ax       
      mov dx, 0x1f0
  .go_on_read:
      in ax,dx        
      mov [ebx], ax
      add ebx, 2
              ; 由于在实模式下偏移地址为16位,所以用bx只会访问到0~FFFFh的偏移。
              ; loader的栈指针为0x900,bx为指向的数据输出缓冲区,且为16位，
              ; 超过0xffff后,bx部分会从0开始,所以当要读取的扇区数过大,待写入的地址超过bx的范围时，
              ; 从硬盘上读出的数据会把0x0000~0xffff的覆盖，
              ; 造成栈被破坏,所以ret返回时,返回地址被破坏了,已经不是之前正确的地址,
              ; 故程序出会错,不知道会跑到哪里去。
              ; 所以改为ebx代替bx指向缓冲区,这样生成的机器码前面会有0x66和0x67来反转。
              ; 0X66用于反转默认的操作数大小! 0X67用于反转默认的寻址方式.
              ; cpu处于16位模式时,会理所当然的认为操作数和寻址都是16位,处于32位模式时,
              ; 也会认为要执行的指令是32位.
              ; 当我们在其中任意模式下用了另外模式的寻址方式或操作数大小(姑且认为16位模式用16位字节操作数，
              ; 32位模式下用32字节的操作数)时,编译器会在指令前帮我们加上0x66或0x67，
              ; 临时改变当前cpu模式到另外的模式下.
              ; 假设当前运行在16位模式,遇到0X66时,操作数大小变为32位.
              ; 假设当前运行在32位模式,遇到0X66时,操作数大小变为16位.
              ; 假设当前运行在16位模式,遇到0X67时,寻址方式变为32位寻址
              ; 假设当前运行在32位模式,遇到0X67时,寻址方式变为16位寻址.

      loop .go_on_read
      ret
</code></pre>
<p>注意，在内核页表中，我们仍置US位为U，是因为内核的加载程序是运行在用户特权下的。由于我们目前只需要1MB的物理内存，每个页能映射4kB，因此只需要创建256个普通页表项，普通页表目录也只需要一个，并且这1MB的内存中，虚拟地址就等于物理地址。另外，第一个内核页表目录也指向256个普通页表项，因为我们需要让内核在这1MB的物理内存下被加载运行，之后的那1GB内核虚拟内存的页表和页表目录创建时就把P位置为0，表示他们不存在于内存中。</p>
<p>在加载完页表之后，我们就可以把控制寄存器CR0的第31位置为1，表示让CPU开启虚拟寻址模式，然后重新将全局描述符表加载到内核区域，再将内核加载到内核区的内存中就可以运行操作系统内核了。</p>
<h3 id="载入内核程序"><a href="#载入内核程序" class="headerlink" title="载入内核程序"></a>载入内核程序</h3><p>在载入内核之前，首先我们要了解ELF文件格式，ELF的E和L就是executable and linkable的缩写，一个ELF文件在链接或者执行视图中可以分段(segment)或者分节(section)：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/01-elf.png" alt="elf" class="lazyload"></p>
<p>elf的header是一个数据结构，用来记录这个ELF文件的信息：</p>
<pre><code class="lang-assembly">/* 32位elf头 */
struct Elf32_Ehdr
&#123;
    unsigned char e_ident[16];
    Elf32_Half e_type;
    Elf32_Half e_machine;
    Elf32_Word e_version;
    Elf32_Addr e_entry;
    Elf32_Off e_phoff;
    Elf32_Off e_shoff;
    Elf32_Word e_flags;
    Elf32_Half e_ehsize;
    Elf32_Half e_phentsize;
    Elf32_Half e_phnum;
    Elf32_Half e_shentsize;
    Elf32_Half e_shnum;
    Elf32_Half e_shstrndx;
&#125;;
</code></pre>
<p>其中，<code>e_indent[16]</code>功能如下：</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/kernel-note/01-e16-indent.png" alt="e16-indent" class="lazyload"></p>
<p><code>e_type</code>占2字节，表示elf目标文件类型，一共有下面几种：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>elf目标文件类型</th>
<th>取值</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>ET_NONE</td>
<td>0</td>
<td>未知目标文件格式</td>
</tr>
<tr>
<td>ET_REL</td>
<td>1</td>
<td>可重定位文件</td>
</tr>
<tr>
<td>ET_EXEC</td>
<td>2</td>
<td>可执行文件</td>
</tr>
<tr>
<td>ET_DYN</td>
<td>3</td>
<td>动态共享目标文件</td>
</tr>
<tr>
<td>ET_CORE</td>
<td>4</td>
<td>core文件，即程序崩溃时其内存映像的转储格式</td>
</tr>
<tr>
<td>ET_LOPROC</td>
<td>0xff00</td>
<td>特定处理器文件的扩展下边界</td>
</tr>
<tr>
<td>ET_HIPROC</td>
<td>0xffff</td>
<td>特定处理器文件的扩展上边界</td>
</tr>
</tbody>
</table>
</div>
<p>其余的字段意义如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>字段</th>
<th>大小(字节)</th>
<th>意义</th>
</tr>
</thead>
<tbody>
<tr>
<td>e_machine</td>
<td>2</td>
<td>支持的硬件平台</td>
</tr>
<tr>
<td>e_version</td>
<td>4</td>
<td>表示版本信息</td>
</tr>
<tr>
<td>e_entry</td>
<td>4</td>
<td>操作系统运行该程序时，将控制权转交到的虚拟地址</td>
</tr>
<tr>
<td>e_phoff</td>
<td>4</td>
<td>程序头表在文件内的字节偏移量。如果没有程序头表，该值为0</td>
</tr>
<tr>
<td>e_shoff</td>
<td>4</td>
<td>节头表在文件内的字节偏移量。若没有节头表，该值为0</td>
</tr>
<tr>
<td>e_flags</td>
<td>4</td>
<td>与处理器相关的标志</td>
</tr>
<tr>
<td>e_ehsize</td>
<td>2</td>
<td>指明 elf header 的字节大小</td>
</tr>
<tr>
<td>e_phentsize</td>
<td>2</td>
<td>指明程序头表(program header table )中每个条目(entry)的字节大小</td>
</tr>
<tr>
<td>e_phnum</td>
<td>2</td>
<td>指明程序头表中条目的数量。实际上就是段的个数</td>
</tr>
<tr>
<td>e_shentsize</td>
<td>2</td>
<td>节头表中每个条目的字节大小，即每个用来描述节信息的数据结构的字节大小</td>
</tr>
<tr>
<td>e_shnum</td>
<td>2</td>
<td>指明节头表中条目的数量。实际上就是节的个数</td>
</tr>
<tr>
<td>e_shstrndx</td>
<td>2</td>
<td>指明 string name table 在节头表中的索引 index</td>
</tr>
</tbody>
</table>
</div>
<p>在加载程序中，我们需要做的就是将内核按照编译好的虚拟地址将各个段复制到对应的位置，然后jump到内核的运行入口（链接时可用指定）去开启内核，然后loader的生命历程就结束了。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><h3 id="虚拟机bochs的安装与配置"><a href="#虚拟机bochs的安装与配置" class="headerlink" title="虚拟机bochs的安装与配置"></a>虚拟机bochs的安装与配置</h3><p>这里使用2.6.2版本，下载地址： <a target="_blank" rel="noopener" href="https://sourceforge.net/projects/bochs/files/bochs/2.6.2/">https://sourceforge.net/projects/bochs/files/bochs/2.6.2/</a></p>
<p>下载源代码文件之后解压进入目录，然后配置：</p>
<pre><code class="lang-shell">./configure \
--prefix=/*你的安装目录*/ \
--enable-debugger \
--enable-disasm \
--enable-iodebug \
--enable-x86-debugger \
--with-x \
--with-x11
</code></pre>
<p>然后<code>make</code>,如果报错说</p>
<pre><code class="lang-shell">Makefile:179: recipe for target &#39;bochs&#39; failed
make: *** [bochs] Error 1
</code></pre>
<p>就再makefile里找到<code>LIBS =</code>，尾部加上<code>-lpthread</code>,注意这里不要再configure，否则makefile会被覆盖，再<code>make</code>，<code>make install</code>就可以了</p>
<p>进入bochs的目录，然后配置文件<code>bochsrc.disk</code>：</p>
<pre><code># Configuration file for Bochs
# 设置Bochs在运行过程中能够使用的内存: 32 MB
megs: 32

# 设置真实机器的BIOS和VGA BIOS
# 修改成你们对应的地址

romimage: file=*bochs的目录*/share/bochs/BIOS-bochs-latest
vgaromimage: file=*bochs的目录*/bochs-2.6.2/share/bochs/VGABIOS-lgpl-latest

# 设置Bochs所使用的磁盘
# 设置启动盘符
boot: disk

# 设置日志文件的输出
log: bochs.out

# 开启或关闭某些功能，修改成你们对应的地址
mouse: enabled=0
keyboard:keymap=*bochs的目录*/share/bochs/keymaps/x11-pc-us.map

# 硬盘设置
ata0: enabled=1, ioaddr1=0x1f0, ioaddr2=0x3f0, irq=14

# 增加gdb支持，这里添加会报错，暂时不需要
# gdbstub: enabled=1, port=1234, text_base=0, data_base=0, bss_base=0
</code></pre><p>然后运行<code>bin/bximage</code>，创建一个60M的虚拟硬盘，遇到选项全部回车，然后问size的时候填个60，然后把让你复制的这一行：<code>ata0-master: type=disk, path=&quot;hd60M.img&quot;, mode=flat, cylinders=121, heads=16, spt=63</code>复制到配置文件中，记得path改成绝对路径，bochs不认识相对路径</p>
<p>之后运行bochs就完事了</p>

            
        </article>
    </div>
    
    <div class="nexmoe-post">
        <a href="/2020/08/14/glibc%20malloc%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">
            
                <div class="nexmoe-post-cover mdui-ripple" style="padding-bottom: 62.5%;"> 
                    <img data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/social_app_add_code/wgwall.jpg" data-sizes="auto" alt="glibc malloc 源码分析" class="lazyload">
                    <h1>glibc malloc 源码分析</h1>
                </div>
            
        </a>

        <div class="nexmoe-post-meta nexmoe-rainbow">
            <a><i class="nexmoefont icon-calendar-fill"></i>2020年08月14日</a>
            <a><i class="nexmoefont icon-areachart"></i>5k 字</a>
            <a><i class="nexmoefont icon-time-circle-fill"></i>大概 25 分钟</a>
        </div>

        <article>
            
                <h1 id="glibc-malloc-源码分析"><a href="#glibc-malloc-源码分析" class="headerlink" title="glibc malloc 源码分析"></a>glibc malloc 源码分析</h1><p>linux给了我们两种类型的系统调用来申请动态内存，分别是<code>brk()</code>和<code>mmap()</code>，<code>malloc()</code>仅仅是在这二者之上做了一些其他的事情而已，这里从源代码来剖析一下<code>glibc malloc</code>都做了什么。源代码是glibc v2.32版本。</p>
<h2 id="chunk"><a href="#chunk" class="headerlink" title="chunk"></a>chunk</h2><p>‘chunk’指的就是<code>malloc</code>分配内存的最小单元，我们来看下它的数据结构：</p>
<pre><code class="lang-c">/*
  This struct declaration is misleading (but accurate and necessary).
  It declares a &quot;view&quot; into memory allowing access to necessary
  fields at known offsets from a given base. See explanation below.
*/

struct malloc_chunk &#123;

  INTERNAL_SIZE_T      mchunk_prev_size;  /* Size of previous chunk (if free).  */
  INTERNAL_SIZE_T      mchunk_size;       /* Size in bytes, including overhead. */

  struct malloc_chunk* fd;         /* double links -- used only if free. */
  struct malloc_chunk* bk;

  /* Only used for large blocks: pointer to next larger size.  */
  struct malloc_chunk* fd_nextsize; /* double links -- used only if free. */
  struct malloc_chunk* bk_nextsize;
&#125;;
</code></pre>
<p>首先<code>INTERNAL_SIZE_T</code>其实就是无符号整数<code>size_t</code>：x86-64 linux下，32位操作系统为4字节32位，64位操作系统为64位8字节，用下面的这个宏定义：</p>
<pre><code class="lang-c">#ifndef INTERNAL_SIZE_T
# define INTERNAL_SIZE_T size_t
#endif
</code></pre>
<p>这里面的4根指针注释上都强调了<strong>只有在free了后才使用</strong>，因此使用中的chunk是长这样的：</p>
<pre><code class="lang-c">    chunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of previous chunk, if unallocated (P clear)  |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of chunk, in bytes                     |A|M|P|
      mem-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             User data starts here...                          .
        .                                                               .
        .             (malloc_usable_size() bytes)                      .
        .                                                               |
nextchunk-&gt; +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             (size of chunk, but used for application data)    |
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
        |             Size of next chunk, in bytes                |A|0|1|
        +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
</code></pre>
<p>这里注意，<code>mchunk_size</code>最后面的3个bit是用来表示这个chunk的一些信息的，意义如下:</p>
<ul>
<li>A表示<code>NON_MAIN_ARENA</code>，记录当前 chunk 是否不属于主线程，1 表示不属于，0 表示属于。</li>
<li>M表示<code>IS_MAPPED</code>，记录当前chunk是否由<code>mmap</code>分配的，1表示是，0表示不是。</li>
<li>P表示<code>PREV_INUSE</code>，记录物理上相邻的前一个chunk是否正在使用，1表示正在使用，0表示没有。</li>
</ul>
<p>接下来就是一些chunk的宏函数：</p>
<pre><code class="lang-c">#ifndef MALLOC_ALIGNMENT
#define MALLOC_ALIGNMENT       (2 * SIZE_SZ)
#endif

/* conversion from malloc headers to user pointers, and back */

#define chunk2mem(p)   ((void*)((char*)(p) + 2*SIZE_SZ))
#define mem2chunk(mem) ((mchunkptr)((char*)(mem) - 2*SIZE_SZ))

//#define offsetof(s,m) ((size_t)&amp;(((s*)0)-&gt;m))
/* The smallest possible chunk */
#define MIN_CHUNK_SIZE        (offsetof(struct malloc_chunk, fd_nextsize))

/* The smallest size we can malloc is an aligned minimal chunk */

#define MINSIZE  \
  (unsigned long)(((MIN_CHUNK_SIZE+MALLOC_ALIGN_MASK) &amp; ~MALLOC_ALIGN_MASK))
</code></pre>
<p>其中：</p>
<ul>
<li><p><code>chunk2mem(p)</code>：偏移<code>2*SIZE_SZ</code>到用户真正使用的数据区</p>
</li>
<li><p><code>MIN_CHUNK_SIZE</code>:chunk的最小size。在CTF wiki的引用里面<code>MIN_CHUNK_SIZE</code>的定义是：</p>
<pre><code class="lang-c">#define MIN_CHUNK_SIZE (offsetof(struct malloc_chunk, fd_nextsize))
</code></pre>
<p>这里<code>offsetof()</code>函数返回的是一个<code>size_t</code>，大小为结构成员相对于结构开头的偏移量，在64位操作系统下，<code>MIN_CHUNK_SIZE</code>是32，32位操作系统下是16。</p>
</li>
<li><p><code>MINSIZE</code>：申请最小的堆内存大小，展开后和<code>MIN_CHUNK_SIZE</code>一样。（虽然是个无关紧要的细节，但我没看懂为什么要定义相同的<code>MIN_CHUNK_SIZE</code>和<code>MINSIZE</code>）</p>
</li>
</ul>
<p>检查对齐的宏：</p>
<pre><code class="lang-c">/* Check if m has acceptable alignment */

#define aligned_OK(m)  (((unsigned long)(m) &amp; MALLOC_ALIGN_MASK) == 0)

//SIZE_SZ = sizeof(size_t)
#define misaligned_chunk(p) \
  ((uintptr_t)(MALLOC_ALIGNMENT == 2 * SIZE_SZ ? (p) : chunk2mem (p)) \
   &amp; MALLOC_ALIGN_MASK)
</code></pre>
<p>这里可以看出，申请内存大小必须是<code>2*SIZE_SZ</code>的整数倍，否则也会给你对齐到整数倍。</p>
<p>然后是把malloc请求的size转换为对应chunk的size宏和对request size做检查的宏：</p>
<pre><code class="lang-c">/* pad request bytes into a usable size -- internal version */

#define request2size(req)                                         \
  (((req) + SIZE_SZ + MALLOC_ALIGN_MASK &lt; MINSIZE)  ?             \
   MINSIZE :                                                      \
   ((req) + SIZE_SZ + MALLOC_ALIGN_MASK) &amp; ~MALLOC_ALIGN_MASK)

/* Check if REQ overflows when padded and aligned and if the resulting value
   is less than PTRDIFF_T.  Returns TRUE and the requested size or MINSIZE in
   case the value is less than MINSIZE on SZ or false if any of the previous
   check fail.  */
static inline bool
checked_request2size (size_t req, size_t *sz) __nonnull (1)
&#123;
  if (__glibc_unlikely (req &gt; PTRDIFF_MAX))
    return false;
  *sz = request2size (req);
  return true;
&#125;
</code></pre>
<p>接下来是对chunk做一些操作的宏，从命名和定义就可以看出具体用途：</p>
<pre><code class="lang-c">/* size field is or&#39;ed with PREV_INUSE when previous adjacent chunk in use */
#define PREV_INUSE 0x1
/* extract inuse bit of previous chunk */
#define prev_inuse(p)       ((p)-&gt;mchunk_size &amp; PREV_INUSE)
/* size field is or&#39;ed with IS_MMAPPED if the chunk was obtained with mmap() */
#define IS_MMAPPED 0x2
/* check for mmap()&#39;ed chunk */
#define chunk_is_mmapped(p) ((p)-&gt;mchunk_size &amp; IS_MMAPPED)
/* size field is or&#39;ed with NON_MAIN_ARENA if the chunk was obtained
   from a non-main arena.  This is only set immediately before handing
   the chunk to the user, if necessary.  */
#define NON_MAIN_ARENA 0x4
/* Check for chunk from main arena.  */
#define chunk_main_arena(p) (((p)-&gt;mchunk_size &amp; NON_MAIN_ARENA) == 0)
/* Mark a chunk as not being on the main arena.  */
#define set_non_main_arena(p) ((p)-&gt;mchunk_size |= NON_MAIN_ARENA)
/*
   Bits to mask off when extracting size

   Note: IS_MMAPPED is intentionally not masked off from size field in
   macros for which mmapped chunks should never be seen. This should
   cause helpful core dumps to occur if it is tried by accident by
   people extending or adapting this malloc.
 */
#define SIZE_BITS (PREV_INUSE | IS_MMAPPED | NON_MAIN_ARENA)
/* Get size, ignoring use bits */
#define chunksize(p) (chunksize_nomask (p) &amp; ~(SIZE_BITS))
/* Like chunksize, but do not mask SIZE_BITS.  */
#define chunksize_nomask(p)         ((p)-&gt;mchunk_size)
/* Ptr to next physical malloc_chunk. */
#define next_chunk(p) ((mchunkptr) (((char *) (p)) + chunksize (p)))
/* Size of the chunk below P.  Only valid if !prev_inuse (P).  */
#define prev_size(p) ((p)-&gt;mchunk_prev_size)
/* Set the size of the chunk below P.  Only valid if !prev_inuse (P).  */
#define set_prev_size(p, sz) ((p)-&gt;mchunk_prev_size = (sz))
/* Ptr to previous physical malloc_chunk.  Only valid if !prev_inuse (P).  */
#define prev_chunk(p) ((mchunkptr) (((char *) (p)) - prev_size (p)))
/* Treat space at ptr + offset as a chunk */
#define chunk_at_offset(p, s)  ((mchunkptr) (((char *) (p)) + (s)))
/* extract p&#39;s inuse bit */
#define inuse(p)                                  \
  ((((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size) &amp; PREV_INUSE)
/* set/clear chunk as being inuse without otherwise disturbing */
#define set_inuse(p)                                  \
  ((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size |= PREV_INUSE
#define clear_inuse(p)                                  \
  ((mchunkptr) (((char *) (p)) + chunksize (p)))-&gt;mchunk_size &amp;= ~(PREV_INUSE)
/* check/set/clear inuse bits in known places */
#define inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size &amp; PREV_INUSE)
#define set_inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size |= PREV_INUSE)
#define clear_inuse_bit_at_offset(p, s)                          \
  (((mchunkptr) (((char *) (p)) + (s)))-&gt;mchunk_size &amp;= ~(PREV_INUSE))
/* Set size at head, without disturbing its use bit */
#define set_head_size(p, s)  ((p)-&gt;mchunk_size = (((p)-&gt;mchunk_size &amp; SIZE_BITS) | (s)))
/* Set size/use field */
#define set_head(p, s)       ((p)-&gt;mchunk_size = (s))
/* Set size at footer (only when chunk is not in use) */
#define set_foot(p, s)       (((mchunkptr) ((char *) (p) + (s)))-&gt;mchunk_prev_size = (s))
</code></pre>
<h2 id="Bin"><a href="#Bin" class="headerlink" title="Bin"></a>Bin</h2><p>前面也说了，chunk是用户通过<code>malloc</code>申请内存的最小单元，glibc malloc不会在一个chunk free了以后马上将内存还给操作系统，而是创建了一些数据结构来接管他们，以节省再次申请时的时间，由于时间空间局部性的存在，这种设计一般来说是能起作用的（当然对于生命周期很长的程序这样的设计可能导致一大堆内存释放不掉）。在内部为了管理这些free chunk，glibc使用了一种叫bins的结构：</p>
<pre><code class="lang-c">#define NBINS 128
typedef struct malloc_chunk *mbinptr;

/* addressing -- note that bin_at(0) does not exist */
#define bin_at(m, i) \
  (mbinptr) (((char *) &amp;((m)-&gt;bins[((i) - 1) * 2]))                  \
             - offsetof (struct malloc_chunk, fd))

/* analog of ++bin */
#define next_bin(b)  ((mbinptr) ((char *) (b) + (sizeof (mchunkptr) &lt;&lt; 1)))

/* Reminders about list directionality within bins */
#define first(b)     ((b)-&gt;fd)
#define last(b)      ((b)-&gt;bk)
/* Normal bins packed as described above */
mchunkptr bins[NBINS * 2 - 2];
</code></pre>
<p>这里就能看出来，bins是一个chunk指针组成的数组，而前面chunk的数据结构定义中也有指向前一个free chunk和后一个free chunk的指针(如果这个chunk也是free的话)，也就是说，bins实际上就是一个管理free chunk的链表数组。在bins中，如果两个free chunk是物理相邻的，两个chunk就会合并以减少内存碎片，相似地，如果在bins里找不到<code>malloc</code>要的size大小的chunk，那么就会从大chunk中分割出一个符合size要求的chunk来，这个步骤后面的代码会看到。</p>
<p>bins又细分为<code>fastbin</code>，<code>smallbin</code>，<code>largebin</code>和<code>unsortedbin</code>，free chunk会根据一些规则被分到这4个组里。</p>
<h3 id="Fast-Bin"><a href="#Fast-Bin" class="headerlink" title="Fast Bin"></a>Fast Bin</h3><p>首先，小size的chunk在free后如果物理相邻就会被合并，但很多程序常常会申请和释放小内存块，要是每次<code>malloc</code>或者<code>free</code>小块都会进行合并和分割就会导致程序变慢，为了照顾着一些很常用的小块内存，fast bins就出现了：</p>
<pre><code class="lang-c">typedef struct malloc_chunk *mfastbinptr;
//indexing
#define fastbin(ar_ptr, idx) ((ar_ptr)-&gt;fastbinsY[idx])

/* offset 2 to use otherwise unindexable first 2 bins */
#define fastbin_index(sz) \
  ((((unsigned int) (sz)) &gt;&gt; (SIZE_SZ == 8 ? 4 : 3)) - 2)
/* The maximum fastbin request size we support */
#define MAX_FAST_SIZE     (80 * SIZE_SZ / 4)

#define NFASTBINS  (fastbin_index (request2size (MAX_FAST_SIZE)) + 1)
/* Fastbins */
mfastbinptr fastbinsY[NFASTBINS];
</code></pre>
<p><code>NFASTBINS</code>展开以后是10，而根据<code>fastbin_index</code>的定义(这个定义中的-2显然受到了<code>MIN_CHUNK_SIZE</code>的约束)，0和1的index不存在，因此<code>fastbin</code>最多cache 8个chunk，根据这个宏可以推出每个index对应的chunk size大小：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>index</th>
<th>32位系统(SIZE_SZ=4)</th>
<th>64位系统(SIZE_SZ=8)</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>不存在</td>
<td>不存在</td>
</tr>
<tr>
<td>1</td>
<td>不存在</td>
<td>不存在</td>
</tr>
<tr>
<td>2</td>
<td>16</td>
<td>32</td>
</tr>
<tr>
<td>3</td>
<td>24</td>
<td>48</td>
</tr>
<tr>
<td>4</td>
<td>32</td>
<td>64</td>
</tr>
<tr>
<td>5</td>
<td>40</td>
<td>80</td>
</tr>
<tr>
<td>6</td>
<td>48</td>
<td>96</td>
</tr>
<tr>
<td>7</td>
<td>56</td>
<td>112</td>
</tr>
<tr>
<td>8</td>
<td>64</td>
<td>128</td>
</tr>
<tr>
<td>9</td>
<td>72</td>
<td>144</td>
</tr>
</tbody>
</table>
</div>
<p>注意，在fast bins中的free chunk是LIFO的，使用单向链表实现，fast bins能fast也是基于时间空间局部性。在malloc申请一个chunk时，首先就会在fast bins中查找有没有适合的size，如果没用才会进行后面的操作：</p>
<pre><code class="lang-c">/* Set if the fastbin chunks contain recently inserted free blocks.  */
/* Note this is a bool but not all targets support atomics on booleans.  */
int have_fastchunks;
</code></pre>
<p>BTW，fast bins中的chunk会被标记为使用中，即链表中chunk的<code>PREV_INUSE</code>都会被设置为1，为了防止被合并。</p>
<h3 id="Small-bin"><a href="#Small-bin" class="headerlink" title="Small bin"></a>Small bin</h3><p>顾名思义，small bins就是包含着小size chunk的bins：</p>
<pre><code class="lang-c">#define NBINS             128
#define NSMALLBINS         64
#define SMALLBIN_WIDTH    MALLOC_ALIGNMENT
#define SMALLBIN_CORRECTION (MALLOC_ALIGNMENT &gt; 2 * SIZE_SZ)
#define MIN_LARGE_SIZE    ((NSMALLBINS - SMALLBIN_CORRECTION) * SMALLBIN_WIDTH)

#define in_smallbin_range(sz)  \
  ((unsigned long) (sz) &lt; (unsigned long) MIN_LARGE_SIZE)

#define smallbin_index(sz) \
  ((SMALLBIN_WIDTH == 16 ? (((unsigned) (sz)) &gt;&gt; 4) : (((unsigned) (sz)) &gt;&gt; 3))\
   + SMALLBIN_CORRECTION)
</code></pre>
<p>从源码中可以看出，一个chunk是small还是large，是由宏<code>MIN_LARGE_SIZE</code>决定的，这个size在64位操作系统上是1024，在32位系统上是512，小于它的被定义为small chunk，大于等于的是large chunk。</p>
<p>而根据<code>smallbin_index(sz)</code>的indexing规则，32位系统下(<code>SMALLBIN_WIDTH != 16</code>)为<code>sz/8</code>，64位下为<code>sz/16</code>，在已知<code>sz</code>必须和<code>2 * SIZE_SZ</code>(64位操作系统为16，32位操作系统为8)对齐的情况下，那么我们就能反推出small chunk总共的index数量为1024/16=64，正好和<code>NSMALLBINS</code>对上了！</p>
<p>然后我们就可以得到不同small bin的index下，每个size的chunk的一一对应关系,注意<code>MIN_CHUNK_SIZE</code>规定了最小的size，因此index是1和0的情况是不存在的,所以<strong>实际情况上，small bins有62个index</strong>！</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>index</th>
<th>32位系统(SIZE_SZ=4)</th>
<th>64位系统(SIZE_SZ=8)</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>16</td>
<td>32</td>
</tr>
<tr>
<td>3</td>
<td>24</td>
<td>48</td>
</tr>
<tr>
<td>4</td>
<td>32</td>
<td>64</td>
</tr>
<tr>
<td>x</td>
<td>2<em>4</em>x</td>
<td>2<em>8</em>x</td>
</tr>
<tr>
<td>63</td>
<td>504</td>
<td>1008</td>
</tr>
</tbody>
</table>
</div>
<p>fast bin是和small bin的范围有重合的，实际上，<strong>fast bins就是small bins的cache</strong>。</p>
<h3 id="Large-Bin"><a href="#Large-Bin" class="headerlink" title="Large Bin"></a>Large Bin</h3><p>搞懂了small bins，large bins就很简单了，前面说过，比<code>MIN_LARGE_SIZE</code>大的chunk都称为large bins，它们是如何indexing的就看源代码怎么定义的了：</p>
<pre><code class="lang-c">#define largebin_index_32(sz)                                                \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 38) ?  56 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

#define largebin_index_32_big(sz)                                            \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 45) ?  49 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

// XXX It remains to be seen whether it is good to keep the widths of
// XXX the buckets the same or whether it should be scaled by a factor
// XXX of two as well.
#define largebin_index_64(sz)                                                \
  (((((unsigned long) (sz)) &gt;&gt; 6) &lt;= 48) ?  48 + (((unsigned long) (sz)) &gt;&gt; 6) :\
   ((((unsigned long) (sz)) &gt;&gt; 9) &lt;= 20) ?  91 + (((unsigned long) (sz)) &gt;&gt; 9) :\
   ((((unsigned long) (sz)) &gt;&gt; 12) &lt;= 10) ? 110 + (((unsigned long) (sz)) &gt;&gt; 12) :\
   ((((unsigned long) (sz)) &gt;&gt; 15) &lt;= 4) ? 119 + (((unsigned long) (sz)) &gt;&gt; 15) :\
   ((((unsigned long) (sz)) &gt;&gt; 18) &lt;= 2) ? 124 + (((unsigned long) (sz)) &gt;&gt; 18) :\
   126)

#define largebin_index(sz) \
  (SIZE_SZ == 8 ? largebin_index_64 (sz)                                     \
   : MALLOC_ALIGNMENT == 16 ? largebin_index_32_big (sz)                     \
   : largebin_index_32 (sz))

#define bin_index(sz) \
  ((in_smallbin_range (sz)) ? smallbin_index (sz) : largebin_index (sz))
</code></pre>
<p>看一下这个嵌套三元表达式的宏就知道，large bins一共分为了6组，每组的index个数可以从移位算符得出，算一算就可以知道它们一一对应到表里，<em>glibc内存管理ptmalloc源码分析</em>里有完整的数据，这里给出CTF wiki的比较简短的总结：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>组号</th>
<th>index个数</th>
<th>公差</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>6</td>
<td>64</td>
</tr>
<tr>
<td>2</td>
<td>16</td>
<td>512</td>
</tr>
<tr>
<td>3</td>
<td>8</td>
<td>4096</td>
</tr>
<tr>
<td>4</td>
<td>4</td>
<td>32768</td>
</tr>
<tr>
<td>5</td>
<td>2</td>
<td>262144</td>
</tr>
<tr>
<td>6</td>
<td>1</td>
<td>不限制</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Unsorted-Bin"><a href="#Unsorted-Bin" class="headerlink" title="Unsorted Bin"></a>Unsorted Bin</h3><p>源码定义如下：</p>
<pre><code class="lang-c">/* The otherwise unindexable 1-bin is used to hold unsorted chunks. */
#define unsorted_chunks(M)          (bin_at (M, 1))
</code></pre>
<p>可见unsorted bin定义在了bins的第一个index下，因此unsorted bin只是一个链表。</p>
<h3 id="Top-Chunk"><a href="#Top-Chunk" class="headerlink" title="Top Chunk"></a>Top Chunk</h3><p>glibc对top chunk定义和描述如下：</p>
<pre><code class="lang-c">/*
   Top

    The top-most available chunk (i.e., the one bordering the end of
    available memory) is treated specially. It is never included in
    any bin, is used only if no other chunk is available, and is
    released back to the system if it is very large (see
    M_TRIM_THRESHOLD).  Because top initially
    points to its own bin with initial zero size, thus forcing
    extension on the first malloc request, we avoid having any special
    code in malloc to check whether it even exists yet. But we still
    need to do so when getting memory from system, so we make
    initial_top treat the bin as a legal but unusable chunk during the
    interval between initialization and the first call to
    sysmalloc. (This is somewhat delicate, since it relies on
    the 2 preceding words to be zero during this interval as well.)
 */

/* Conveniently, the unsorted bin can be used as dummy top on first call */
#define initial_top(M)              (unsorted_chunks (M))
</code></pre>
<p>根据注释描述，所谓的top chunk就是位于可用堆内存地址最高位的chunk，它不属于任何bin，只有当没有chunk可用时它才会向系统去申请扩展heap的可用区域。为防止被合并，top chunk的<code>prev_inuse</code>始终为1。初始情况时，unsorted chunks用作top chunk。</p>
<p>现在总结一下，宏<code>NBINS</code>告诉我们bins一共有108个入口，small bins有62个，large bins一共有63个，加起来125个bin，根据bin的indexing宏<code>bin_at</code>的定义，<code>bin[0]</code>和<code>bin[127]</code>是不存在的，因此<code>bin[1]</code>就是top chunk，也是unsorted bin，加起来总共126个。</p>
<p>BTW，<code>bins</code>的定义是：</p>
<pre><code class="lang-c">mchunkptr bins[NBINS * 2 - 2];
</code></pre>
<p>也就是实际size有<code>NBINS * 2 - 2</code>一共254个<code>mchunkptr</code>，而chunk实例的是6个<code>mchunkptr</code>的大小，这怎么存的下呢？但我们注意到，<code>bins</code>中的chunk是头节点，那么chunk中的<code>mchunk_prev_size</code>和<code>mchunk_size</code>是没有意义的！而<code>fd_nextsize</code>和<code>bk_nextsize</code>只有large chunk才会用到，那么出于节省内存的想法，我们只需要2个<code>mchunkptr</code>，总共需要的就是126*2=254个<code>mchunkptr</code>的大小，正好对上了！</p>
<p><img data-fancybox="gallery" data-sizes="auto" data-src="https://chenyu-blog-img-1302348848.cos.ap-shanghai.myqcloud.com/glibc-malloc/bin.png" alt="bins" class="lazyload"></p>
<p>总之，glibc的malloc使用的内存管理方法就是链表数组的内存池，和gnu C++远古版本std allocator是相同的思想，因此在后面版本的gnu C++里面默认allocator都是封装malloc，如果看了侯捷的STL源码剖析，别被书里推崇无比的内存池allocator误导了。</p>
<h2 id="其他核心结构"><a href="#其他核心结构" class="headerlink" title="其他核心结构"></a>其他核心结构</h2><h3 id="malloc-state"><a href="#malloc-state" class="headerlink" title="malloc_state"></a>malloc_state</h3><p>定义如下：</p>
<pre><code class="lang-c">struct malloc_state
&#123;
  /* Serialize access.  */
  __libc_lock_define (, mutex);

  /* Flags (formerly in max_fast).  */
  int flags;

  /* Set if the fastbin chunks contain recently inserted free blocks.  */
  /* Note this is a bool but not all targets support atomics on booleans.  */
  int have_fastchunks;

  /* Fastbins */
  mfastbinptr fastbinsY[NFASTBINS];

  /* Base of the topmost chunk -- not otherwise kept in a bin */
  mchunkptr top;

  /* The remainder from the most recent split of a small request */
  mchunkptr last_remainder;

  /* Normal bins packed as described above */
  mchunkptr bins[NBINS * 2 - 2];

  /* Bitmap of bins */
  unsigned int binmap[BINMAPSIZE];

  /* Linked list */
  struct malloc_state *next;

  /* Linked list for free arenas.  Access to this field is serialized
     by free_list_lock in arena.c.  */
  struct malloc_state *next_free;

  /* Number of threads attached to this arena.  0 if the arena is on
     the free list.  Access to this field is serialized by
     free_list_lock in arena.c.  */
  INTERNAL_SIZE_T attached_threads;

  /* Memory allocated from the system in this arena.  */
  INTERNAL_SIZE_T system_mem;
  INTERNAL_SIZE_T max_system_mem;
&#125;;
</code></pre>
<p>这里就可以看到，前面所说的bins是<code>malloc_state</code>的一部分，因此一个<code>malloc_state</code>的实例就是一个分配区域，下面一一说明这些变量：</p>
<p>flags是一些标志位，它的用途从后面的宏定义就可以看出来：</p>
<pre><code class="lang-c">/*
   NONCONTIGUOUS_BIT indicates that MORECORE does not return contiguous
   regions.  Otherwise, contiguity is exploited in merging together,
   when possible, results from consecutive MORECORE calls.

   The initial value comes from MORECORE_CONTIGUOUS, but is
   changed dynamically if mmap is ever used as an sbrk substitute.
 */

#define NONCONTIGUOUS_BIT     (2U)

#define contiguous(M)          (((M)-&gt;flags &amp; NONCONTIGUOUS_BIT) == 0)
#define noncontiguous(M)       (((M)-&gt;flags &amp; NONCONTIGUOUS_BIT) != 0)
#define set_noncontiguous(M)   ((M)-&gt;flags |= NONCONTIGUOUS_BIT)
#define set_contiguous(M)      ((M)-&gt;flags &amp;= ~NONCONTIGUOUS_BIT)
</code></pre>
<p>这里就是用<code>flags</code>的第2位来判断<code>MORECORE</code>是否返回了连续的虚拟地址空间，0为是，1为否。实际上<code>MORECORE</code>就是系统调用<code>sbrk()</code>，只是经过了重重包装：</p>
<pre><code class="lang-c">#define MORECORE         (*__morecore) 
void * __default_morecore (ptrdiff_t);
void *(*__morecore)(ptrdiff_t) = __default_morecore;
void *
__default_morecore (ptrdiff_t increment)
&#123;
  void *result = (void *) __sbrk (increment);
  if (result == (void *) -1)
    return NULL;

  return result;
&#125;
</code></pre>
<p><code>have_fastchunk</code>用来表示这个分配区域是否有fast chunk。以前版本的glibc malloc是用flags的第1位来判断是否有fast chunk的，定义宏的方法和<code>contiguous</code>是一样的，但我看的这个版本是在<code>malloc_state</code>定义了一个<code>have_fastchunk</code>来判断，感觉新的版本有点浪费内存资源了。</p>
<p><code>fastbinsY</code>，前面已经说过了，存储fast chunk链表指针的数组。</p>
<p><code>top</code>，指向<code>bins</code> top chunk的指针。</p>
<p><code>last_remainder</code>，指向chunk的指针， 分配区上次分配 small chunk 时，从一个 chunk 中分<br>裂出一个 small chunk 返回给用户， 分裂后的剩余部分形成一个 chunk，<code>last_remainder</code> 就是<br>指向的这个 chunk 。</p>
<p><code>bins</code>：前面说过了，存储chunk的链表指针数组，分为unsorted bin，fast bin，small bin， large bin，<code>bin[0]</code>不存在，<code>bin[1]</code>是unsorted bin。</p>
<p><code>binmap</code>：一个<code>int</code>数组，关于它的用途，可以看下面部分的代码：</p>
<pre><code class="lang-c">/* Conservatively use 32 bits per map word, even if on 64bit system */
#define BINMAPSHIFT      5
#define BITSPERMAP       (1U &lt;&lt; BINMAPSHIFT)
#define BINMAPSIZE       (NBINS / BITSPERMAP)

#define idx2block(i)     ((i) &gt;&gt; BINMAPSHIFT)
#define idx2bit(i)       ((1U &lt;&lt; ((i) &amp; ((1U &lt;&lt; BINMAPSHIFT) - 1))))

#define mark_bin(m, i)    ((m)-&gt;binmap[idx2block (i)] |= idx2bit (i))
#define unmark_bin(m, i)  ((m)-&gt;binmap[idx2block (i)] &amp;= ~(idx2bit (i)))
#define get_binmap(m, i)  ((m)-&gt;binmap[idx2block (i)] &amp; idx2bit (i))
</code></pre>
<p>这里可以看出<code>BINMAPSIZE</code>的值是128/32=4，我们知道<code>int</code>是32位，那么<code>binmap</code>实际上就是一个128位的buffer，这些宏就是在定义每一位对应每一个<code>bins</code>的映射关系，ptmalloc就使用这些位来标记对应bin中是否有free chunk。</p>
<p><code>next</code>：一根指向下一个分配区的指针。</p>
<p><code>next_free</code>：一根指向下一个free分配区的指针。</p>
<p><code>system_mem</code>：记录当前分配去已经分配的内存大小。</p>
<p><code>max_system_mem</code>：记录当前分配去最大能分配的内存大小。</p>
<h3 id="malloc-par"><a href="#malloc-par" class="headerlink" title="malloc_par"></a>malloc_par</h3><p>定义如下：</p>
<pre><code class="lang-c">struct malloc_par
&#123;
  /* Tunable parameters */
  unsigned long trim_threshold;
  INTERNAL_SIZE_T top_pad;
  INTERNAL_SIZE_T mmap_threshold;
  INTERNAL_SIZE_T arena_test;
  INTERNAL_SIZE_T arena_max;

  /* Memory map support */
  int n_mmaps;
  int n_mmaps_max;
  int max_n_mmaps;
  /* the mmap_threshold is dynamic, until the user sets
     it manually, at which point we need to disable any
     dynamic behavior. */
  int no_dyn_threshold;

  /* Statistics */
  INTERNAL_SIZE_T mmapped_mem;
  INTERNAL_SIZE_T max_mmapped_mem;

  /* First address handed out by MORECORE/sbrk.  */
  char *sbrk_base;

#if USE_TCACHE
  /* Maximum number of buckets to use.  */
  size_t tcache_bins;
  size_t tcache_max_bytes;
  /* Maximum number of chunks in each bucket.  */
  size_t tcache_count;
  /* Maximum number of chunks to remove from the unsorted list, which
     aren&#39;t used to prefill the cache.  */
  size_t tcache_unsorted_limit;
#endif
&#125;;
</code></pre>
<p>各个变量的意义如下（摘自ptmalloc源码剖析）：</p>
<p><code>trim_threshold</code>字段表示收缩阈值，默认为 128KB，当每个分配区的 top chunk 大小大于<br>这个阈值时，在一定的条件下，调用 free 时会收缩内存，减小 top chunk 的大小。由于 mmap<br>分配阈值的动态调整，在 free 时可能将收缩阈值修改为 <code>mmap</code> 分配阈值的 2 倍，在 64 位系<br>统上， mmap 分配阈值最大值为 32MB，所以收缩阈值的最大值为 64MB，在 32 位系统上，<br>mmap 分配阈值最大值为 512KB，所以收缩阈值的最大值为 1MB。 收缩阈值可以通过函数<br><code>mallopt()</code>进行设置。</p>
<p><code>top_pad</code> 字段表示在分配内存时是否添加额外的 pad，默认该字段为 0。<br><code>mmap_threshold</code> 字段表示 <code>mmap</code> 分配阈值，默认值为 128KB，在 32 位系统上最大值为<br>512KB， 64 位系统上的最大值为 32MB，由于默认开启 <code>mmap</code> 分配阈值动态调整，该字段的<br>值会动态修改，但不会超过最大值。</p>
<p><code>arena_test</code> 和<code>arena_max</code> 用于 <code>PER_THREAD</code> 优化，在 32 位系统上 <code>arena_test</code>默认值为 2，<br>64 位系统上的默认值为 8， 当每个进程的分配区数量小于等于 <code>arena_test</code> 时，不会重用已有<br>的分配区。为了限制分配区的总数，用 <code>arena_max</code> 来保存分配区的最大数量，当系统中的分<br>配区数量达到 <code>arena_max</code>，就不会再创建新的分配区，只会重用已有的分配区。 这两个字段<br>都可以使用 <code>mallopt()</code>函数设置。</p>
<p><code>n_mmaps</code> 字段表示当前进程使用 <code>mmap()</code>函数分配的内存块的个数。<br><code>n_mmaps_max</code> 字段表示进程使用 <code>mmap()</code>函数分配的内存块的最大数量，默认值为 65536，可以使用 <code>mallopt()</code>函数修改。<br><code>max_n_mmaps</code>字段表示当前进程使用 <code>mmap()</code>函数分配的内存块的数量的最大值，有关<br>系 <code>n_mmaps &lt;= max_n_mmaps</code> 成立。 这个字段是由于 <code>mstats()</code>函数输出统计需要这个字段。<br><code>no_dyn_threshold</code> 字段表示是否开启 <code>mmap</code> 分配阈值动态调整机制，默认值为 0，也就<br>是默认开启<code>mmap</code> 分配阈值动态调整机制。<br><code>pagesize</code> 字段表示系统的页大小，默认为 4KB。<br><code>mmapped_mem</code> 和 <code>max_mmapped_mem</code> 都用于统计<code>mmap</code> 分配的内存大小，一般情况<br>下两个字段的值相等， <code>max_mmapped_mem</code>用于<code>mstats()</code>函数。<br><code>max_total_mem</code> 字段在单线程情况下用于统计进程分配的内存总数。<br><code>sbrk_base</code>字段表示堆的起始地址。</p>
<h2 id="References："><a href="#References：" class="headerlink" title="References："></a>References：</h2><p>[1]. <a target="_blank" rel="noopener" href="https://ctf-wiki.github.io/ctf-wiki/pwn/linux/glibc-heap/heap_structure-zh">https://ctf-wiki.github.io/ctf-wiki/pwn/linux/glibc-heap/heap_structure-zh</a></p>
<p>[2].glibc内存管理ptmalloc源码分析</p>

            
        </article>
    </div>
    
</section>

        <div class="nexmoe-post-right">
          <div class="nexmoe-fixed">
            <div class="nexmoe-tool"> 
              
                
                  
                  
                  
                    
                
              
              <a href="#nexmoe-content" class="toc-link" aria-label="回到顶部" title="top"><button class="mdui-fab mdui-ripple"><i class="nexmoefont icon-caret-top"></i></button></a>
            </div>
          </div>
        </div>
    </div>
  </div>
  <script src="https://cdn.jsdelivr.net/combine/npm/lazysizes@5.1.0/lazysizes.min.js,npm/mdui@0.4.3/dist/js/mdui.min.js?v=1"></script>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/gh/fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js"></script>

 

<script async src="/js/app.js?v=1624344442071"></script>

<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js"></script>
<script>
	$(".justified-gallery").justifiedGallery({
		rowHeight: 160,
		margins: 10,
	});
</script>

  





</body>

</html>
